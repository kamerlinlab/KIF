{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation for KIF - Key Interactions Finder","text":"<p>A python package to identify the key molecular interactions that regulate any conformational change.</p> <p></p>"},{"location":"#package-overview","title":"Package Overview:","text":"<ul> <li>Identify important non-covalent interactions that are associated with any conformational change you are interested in (as long as you can describe the descriptor and sample the descriptor in your MD simulations). The non-covalent interactions are scored according to their association/importance to the conformational change and you can easily convert the per interaction/feature scores to per residue scores as well.</li> <li>Generate PyMOL output scripts that enable you to visualise your results on 3D structures.</li> <li>Generate per residue correlation and distance matrices that can be easily applied to the many graph theory methods available in order to study protein interaction networks and allostery within your system (no descriptor/target variable required for this).</li> </ul> <p>Note that how you define the descriptor is up to you, and you can use either a continuous variable or a categorical variable (some tips on how to decide what to use will be given below).</p> <p>More Detail Please! For a more complete description of KIF, please refer to our article. Included in the article is a description of some of the generic workflows possible alongside the application of KIF to several different biomolecular systems.</p> <p>There are also tutorials available (discussed below).</p>"},{"location":"#the-approximate-workflow-for-this-package-is-as-follows","title":"The approximate workflow for this package is as follows:","text":"<ol> <li>Run MD simulations on your system(s) of interest with whichever MD engine you want.</li> <li>(Both can be done simultaneously)<ol> <li>Analyse the trajectory frames with KIF to determine all non-covalent interactions.* See tutorial 0 for how to do this.</li> <li>(Optional) Calculate the value of a target variable for each frame using whatever approach you see fit (tips provided below).</li> </ol> </li> <li>Load your non-covalent interactions data and optionally generated target data into KIF and perform some combination of machine learning, statistical analysis or network analysis.</li> <li>Output the results for analysis. This includes visualisation scripts which are compatible with PyMOL so you can see the results on a 3D protein structure.</li> </ol> <p>*The previous implementation of KIF used PyContact (note that this program is not made by us), to determine all the non-covalent interactions in your trajectory. You can still use this approach if desired, see the legacy tutorials section below.</p>"},{"location":"#dependencies-and-install","title":"Dependencies and Install","text":"<ul> <li>Python 3.7 or higher is required as this package uses dataclasses. We recommend python 3.10 as 3.7 is rather old.</li> </ul> <p>Option 1: Install with pip <pre><code>pip install KIF\n</code></pre></p> <p>Option 2: Clone/Download Repo first and then run setup.py :</p> <pre><code>cd KIF-main\npython setup.py install\n</code></pre> <p>Note that in prior versions of KIF (less than 0.2.0) you would have needed to also install PyContact (see the repo for how to do this). This is no longer the case.</p>"},{"location":"#choosing-an-appropriate-target-variable","title":"Choosing an Appropriate Target variable.","text":"<p>To perform either the machine learning or statistical analysis methods available in this package you will want to calculate a target variable to go alongside the features (non-covalent interactions identified by KIF). This target variable should as cleanly as possible describe your process of interest.</p> <p>Below are some examples of what could work for you. Of course, this is use case specific so feel free to select what makes sense for your problem.</p> <p>Enzyme catalysis - A per frame reacting atom distance for regression or define a max reacting atom distance cut-off to classify each frame as either \"active\" or \"inactive\".</p> <p>Conformational Changes - Calculate the RMSD for each frame in your simulation to each conformational state. In this case of 2 different states, classify each frame as either \"State 1\", \"State 2\" or \"neither\" based on your RMSD metrics.</p> <p>For example: * if RMSD to \"State 1\" &lt;= 1.5 \u00c5 and an RMSD to \"State 2\" &gt;= 1.5 \u00c5 --&gt; assign frame as \"State 1\". * if RMSD to \"State 1\" =&gt; 1.5 \u00c5 and an RMSD to \"State 2\" &lt;= 1.5 \u00c5 --&gt; assign frame as \"State 2\". * else --&gt; assign frame as \"neither\". You can also consider dropping the frames with state \"neither\" from your analysis to make the calculation cleaner (i.e., turn it into binary classification). This is the approach we took for the enzyme PTP1B, which you can find described in our manuscript.</p>"},{"location":"#tutorials-available","title":"Tutorials Available","text":"<p>All tutorials include the setup and post-processing steps used for each system. All tutorials used datasets we analyzed in our manuscript</p> <ol> <li> <p>identify_contacts.py - Example script showing how you can use KIF to identify all the non-covalent interactions present in your MD simulations. This needs to be determined before using the rest of KIF.</p> </li> <li> <p>Tutorial_PTP1B_Classification_ML_Stats.ipynb   - Perform binary classification ML and statistical analysis on simulations of PTP1B. Used to describe the differences in the closed and open WPD-loop states of PTP1B.</p> </li> <li> <p>Tutorial_KE07_Regression_ML_Stats.ipynb  - Perform regression ML and statistical analysis on a kemp eliminase enzyme. Here the target value is the side chain dihedral of W50.</p> </li> <li> <p>network_analysis_tutorial  - Preparation of PTP1B inputs required for graph theory based calculations. This tutorial is in its own folder, as two additional scripts are provided:</p> <ul> <li>A .R script (which uses BIO3D) to perform WISP</li> <li>A python script to generate PyMOL compatible figures depicting the results from the WISP calculation (The .R script will only generate VMD compatible ones.)</li> </ul> </li> <li> <p>Workup_PDZ_Bootstrapping.ipynb  - Workup the bootstrapping calculations performed on the PDZ3 domain. As described in the notebook, the bootstrapping itself was computationally intensive to run so only the workup is included in this notebook. We have however included the scripts that we used to perform bootstrapping for both PTP1B and the PDZ3 domain on Zenodo</p> </li> </ol> <p>There are also several legacy tutorials which can be found here. These tutorials are almost identical to the equivalent ones found above, but instead use non-covalent interaction data identified with PyContact instead of KIF.</p>"},{"location":"#license-and-disclaimer","title":"License and Disclaimer","text":"<p>This software is published under a GNU General Public License v2.0.</p> <p>As the principal investigator behind this software is employed by Georgia Tech University we must also clarify: \u201cThe software is provided \u201cas is.\u201d Neither the Georgia Institute of Technology nor any of its units or its employees, nor the software developers of KIF or any other person affiliated with the creation, implementation, and upkeep of the software\u2019s code base, knowledge base, and servers (collectively, the \u201cEntities\u201d) shall be held liable for your use of the platform or any data that you enter. The Entities do not warrant or make any representations of any kind or nature with respect to the System, and the Entities do not assume or have any responsibility or liability for any claims, damages, or losses resulting from your use of the platform. None of the Entities shall have any liability to you for use charges related to any device that you use to access the platform or use and receive the platform, including, without limitation, charges for Internet data packages and Personal Computers. THE ENTITIES DISCLAIM ALL WARRANTIES WITH REGARD TO THE SERVICE,INCLUDING WARRANTIES OF MERCHANTABILITY, NON-INFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE TO THE FULLEST EXTENT ALLOWED BY LAW.\u201d</p>"},{"location":"#citing-this-work","title":"Citing this work","text":"<p>If you make use of this package please cite our manuscript:.</p> <p>KIF \u2013 Key Interactions Finder: A Program to Identify the Key Molecular Interactions that Regulate Protein Conformational Changes</p> <p>Authors: Rory M. Crean, Joanna S. G. Slusky, Peter M. Kasson and Shina Caroline Lynn Kamerlin</p> <p>DOI: 10.1063/5.0140882</p>"},{"location":"#issuesquestionscontributions","title":"Issues/Questions/Contributions","text":"<p>All welcome. Please feel free to open an issue or submit a pull request as necessary. Feature requests are welcome too. You can also reach me at: rory.crean [at] kemi.uu.se</p>"},{"location":"chimerax_projections/","title":"ChimeraX Projections","text":"<p>Creates ChimeraX compatible scripts to visualise user generated results on a 3D model of the protein.</p> <p>4 Functions available for the end user:</p> <ol> <li> <p>project_chimerax_per_res_scores(per_res_scores, model_name, out_dir)     Write out a ChimeraX compatible script to project the per residue scores.</p> </li> <li> <p>project_multiple_per_res_scores(all_per_res_scores, out_dir)     Write out multiple ChimeraX compatible visualisation scripts for     the per residue scores, one script for each model used.</p> </li> <li> <p>project_chimerax_top_features()     Write out a ChimeraX compatible script to project the top features.</p> </li> <li> <p>project_multiple_per_feature_scores(all_feature_scores, numb_features, out_dir)     Write out multiple ChimeraX compatible scripts for different models.</p> </li> </ol>"},{"location":"chimerax_projections/#key_interactions_finder.chimerax_projections.get_residue_coordinates","title":"<code>get_residue_coordinates(pdb_file, residue_number)</code>","text":"<p>Get the coordinates of the CA atom of a specified residue from a PDB file.</p> <p>Parameters:</p> Name Type Description Default <code>pdb_file</code> <code>str</code> <p>Path to the PDB file.</p> required <code>residue_number</code> <code>int</code> <p>The residue number.</p> required <p>Returns:</p> Type Description <code>Tuple[float, float, float]</code> <p>The x, y, z coordinates of the CA atom.</p> Source code in <code>key_interactions_finder/chimerax_projections.py</code> <pre><code>def get_residue_coordinates(pdb_file: str, residue_number: int) -&gt; Tuple[float, float, float]:\n    \"\"\"\n    Get the coordinates of the CA atom of a specified residue from a PDB file.\n\n    Parameters\n    ----------\n    pdb_file : str\n        Path to the PDB file.\n    residue_number : int\n        The residue number.\n\n    Returns\n    ----------\n    Tuple[float, float, float]\n        The x, y, z coordinates of the CA atom.\n    \"\"\"\n    # Not relevant warning message for this use case, don't want to scare users.\n    warnings.filterwarnings(action=\"ignore\", message=\"Element information is missing\")\n    u = mda.Universe(pdb_file)\n\n    residue = u.select_atoms(f\"resid {residue_number} and name CA\")\n    if not residue:\n        raise ValueError(f\"Residue number {residue_number} not found in {pdb_file}\")\n    return tuple(residue.positions[0])\n</code></pre>"},{"location":"chimerax_projections/#key_interactions_finder.chimerax_projections.project_chimerax_per_res_scores","title":"<code>project_chimerax_per_res_scores(per_res_scores, model_name='', out_dir='', sphere_color='red')</code>","text":"<p>Write out a ChimeraX compatible script to project the per residue scores.</p> <p>Parameters:</p> Name Type Description Default <code>per_res_scores</code> <code>dict</code> <p>The keys are each residue and values the per residue score.</p> required <code>model_name</code> <code>str</code> <p>Appended to start of output file to identify it.</p> <code>''</code> <code>out_dir</code> <code>str</code> <p>Folder to save outputs to, if none given, saved to current directory.</p> <code>''</code> <code>sphere_color</code> <code>str</code> <p>Color of spheres created per residue.</p> <code>'red'</code> Source code in <code>key_interactions_finder/chimerax_projections.py</code> <pre><code>def project_chimerax_per_res_scores(\n    per_res_scores: dict,\n    model_name: str = \"\",\n    out_dir: str = \"\",\n    sphere_color=\"red\",\n) -&gt; None:\n    \"\"\"\n    Write out a ChimeraX compatible script to project the per residue scores.\n\n    Parameters\n    ----------\n\n    per_res_scores : dict\n        The keys are each residue and values the per residue score.\n\n    model_name : str\n        Appended to start of output file to identify it.\n\n    out_dir : str\n        Folder to save outputs to, if none given, saved to current directory.\n\n    sphere_color : str\n        Color of spheres created per residue.\n    \"\"\"\n    out_dir = _prep_out_dir(out_dir)\n\n    # Header\n    per_res_import_out = \"\"\n    per_res_import_out += \"# 1. Load the PDB file of your system in ChimeraX \\n\"\n    per_res_import_out += \"# 2. Run this script by using the command: open /path/[FILE_NAME.cxc]\\n\"\n    per_res_import_out += \"# 3. Make sure the .cxc file is in the same directory as the pdb.\\n\"\n    # per_res_import_out += f\"open {self.pdb_file}\\n\"\n    per_res_import_out += \"# The lines below are suggestions for potentially nicer figures.\\n\"\n    per_res_import_out += \"# You can comment them in if you want.\\n\"\n    per_res_import_out += \"#set bgColor white\\n\"\n    per_res_import_out += \"#color gray target c\\n\"\n    per_res_import_out += \"#lighting soft\\n\"\n    per_res_import_out += \"#graphics silhouettes true\\n\"\n\n    # Main, tells ChimeraX to show spheres and set their size accordingly.\n    for res_numb, sphere_size in per_res_scores.items():\n        # additional sphere_size to scale better with ChimeraX defaults\n        per_res_import_out += (\n            f\"shape sphere center :{res_numb}@CA radius {sphere_size * 1.5:.4f} color {sphere_color}\\n\"\n        )\n\n    # user selection of all CA carbons so easy to modify the sphere colours etc...\n    # this doesn't work the same in ChimeraX, selects residues but shapes are separate objects\n    # all_spheres_list = list(per_res_scores.keys())\n    # all_spheres_str = ','.join(map(str, all_spheres_list))\n    # per_res_import_out += f\"select :{all_spheres_str}@CA\\n\"\n\n    out_file_name = model_name + \"_ChimeraX_Per_Res_Scores.cxc\"\n    out_file_path = Path(out_dir, out_file_name)\n    _write_file(out_file_path, per_res_import_out)\n    print(f\"The file: {out_file_path} was written to disk.\")\n</code></pre>"},{"location":"chimerax_projections/#key_interactions_finder.chimerax_projections.project_chimerax_top_features","title":"<code>project_chimerax_top_features(per_feature_scores, model_name, pdb_file, numb_features='all', out_dir='')</code>","text":"<p>Write out a ChimeraX compatible script to project the top X features. Features will be shown as cylinders between each residue pair, with cylinder size controlled according to relative score and cylinder colour controlled by interaction type.</p> <p>Parameters:</p> Name Type Description Default <code>per_feature_scores</code> <code>dict</code> <p>Keys are the names of the features and values are their scores.</p> required <code>model_name</code> <code>str</code> <p>What name to appended to the start of the output file name to help identify it.</p> required <code>pdb_file</code> <code>str</code> <p>Path to the PDB file.</p> required <code>numb_features</code> <code>int or str</code> <p>The max number of top scoring features to determine (specified by an int). Alternatively, if set to \"all\", then all feature scores will be determined.</p> <code>'all'</code> <code>out_dir</code> <code>str</code> <p>Folder to save outputs to, if none given, saved to current directory.</p> <code>''</code> Source code in <code>key_interactions_finder/chimerax_projections.py</code> <pre><code>def project_chimerax_top_features(\n    per_feature_scores: dict,\n    model_name: str,\n    pdb_file: str,\n    numb_features: Union[int, str] = \"all\",\n    out_dir: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Write out a ChimeraX compatible script to project the top X features.\n    Features will be shown as cylinders between each residue pair,\n    with cylinder size controlled according to relative score and\n    cylinder colour controlled by interaction type.\n\n    Parameters\n    ----------\n\n    per_feature_scores : dict\n        Keys are the names of the features and values are their scores.\n\n    model_name : str\n        What name to appended to the start of the output file name to help identify it.\n\n    pdb_file : str\n        Path to the PDB file.\n\n    numb_features : int or str\n        The max number of top scoring features to determine (specified by an int).\n        Alternatively, if set to \"all\", then all feature scores will be determined.\n\n    out_dir : str\n        Folder to save outputs to, if none given, saved to current directory.\n    \"\"\"\n    out_dir = _prep_out_dir(out_dir)\n\n    df_feat_import = pd.DataFrame(per_feature_scores.items())\n    df_feat_import_res = df_feat_import[0].str.split(\" \", expand=True)\n\n    res1, res2 = _extract_residue_lists(df_feat_import_res)\n    interact_color = _extract_interaction_types(df_feat_import_res)\n    interact_strengths = _scale_interaction_strengths(df_feat_import)\n\n    # Header of output file.\n    top_feats_out = \"\"\n    top_feats_out += \"# 1. Load the PDB file of your system in ChimeraX \\n\"\n    top_feats_out += \"# 2. Run this script by using the command: open /path/[FILE_NAME.cxc]\\n\"\n    top_feats_out += \"# 3. Make sure the .cxc file is in the same directory as the pdb.\\n\"\n    # top_feats_out += f\"open {pdb_file}\\n\"\n    top_feats_out += \"# The lines below are suggestions for potentially nicer figures.\\n\"\n    top_feats_out += \"# You can comment them in if you want.\\n\"\n    top_feats_out += \"#set bgColor white\\n\"\n    top_feats_out += \"#color gray target c\\n\"\n    top_feats_out += \"#lighting soft\\n\"\n    top_feats_out += \"#graphics silhouettes true\\n\"\n\n    # Main, show CA carbons as spheres and set their size.\n    if numb_features == \"all\":\n        numb_features = len(res1)\n    elif not isinstance(numb_features, int):\n        raise ValueError(\"You defined the parameter 'numb_features' as neither 'all' or as an integer.\")\n\n    # prevent issue if user requests more features than exist.\n    numb_features = min(numb_features, len(res1))\n\n    for i in range(numb_features):\n        coord1 = get_residue_coordinates(pdb_file, res1[i])\n        coord2 = get_residue_coordinates(pdb_file, res2[i])\n        feature_rep = f\"shape cylinder radius {interact_strengths[i]} fromPoint {coord1[0]},{coord1[1]},{coord1[2]} toPoint {coord2[0]},{coord2[1]},{coord2[2]} color {interact_color[i]}\\n\"\n        top_feats_out += feature_rep\n\n    out_file_name = model_name + \"_ChimeraX_Per_Feature_Scores.cxc\"\n    out_file_path = Path(out_dir, out_file_name)\n    _write_file(out_file_path, top_feats_out)\n    print(f\"The file: {out_file_path} was written to disk.\")\n</code></pre>"},{"location":"chimerax_projections/#key_interactions_finder.chimerax_projections.project_multiple_per_feature_scores","title":"<code>project_multiple_per_feature_scores(all_per_feature_scores, pdb_file, numb_features, out_dir='')</code>","text":"<p>Write out multiple ChimeraX compatible scripts for different models.</p> <p>Parameters:</p> Name Type Description Default <code>all_per_feature_scores</code> <code>dict</code> <p>Nested dictionary, the outer layer keys are the model names/methods used. The inner layer is a dict with keys being each residue and values the per residue score.</p> required <code>pdb_file</code> <code>str</code> <p>Path to the PDB file.</p> required <code>numb_features</code> <code>int or str</code> <p>The max number of top scoring features to determine (specified by an int). Alternatively, if set to \"all\", then all per feature scores will be determined.</p> required <code>out_dir</code> <code>str</code> <p>Folder to save outputs to, if none given, saved to current directory.</p> <code>''</code> Source code in <code>key_interactions_finder/chimerax_projections.py</code> <pre><code>def project_multiple_per_feature_scores(\n    all_per_feature_scores: dict,\n    pdb_file: str,\n    numb_features: Union[int, str],\n    out_dir: str = \"\",\n) -&gt; None:\n    \"\"\"\n    Write out multiple ChimeraX compatible scripts for different models.\n\n    Parameters\n    ----------\n\n    all_per_feature_scores : dict\n        Nested dictionary, the outer layer keys are the model names/methods used.\n        The inner layer is a dict with keys being each residue and\n        values the per residue score.\n\n    pdb_file : str\n        Path to the PDB file.\n\n    numb_features : int or str\n        The max number of top scoring features to determine (specified by an int).\n        Alternatively, if set to \"all\", then all per feature scores will be determined.\n\n    out_dir : str\n        Folder to save outputs to, if none given, saved to current directory.\n    \"\"\"\n    for model_name, model_scores in all_per_feature_scores.items():\n        project_chimerax_top_features(\n            per_feature_scores=model_scores,\n            model_name=model_name,\n            pdb_file=pdb_file,\n            numb_features=numb_features,\n            out_dir=out_dir,\n        )\n</code></pre>"},{"location":"chimerax_projections/#key_interactions_finder.chimerax_projections.project_multiple_per_res_scores","title":"<code>project_multiple_per_res_scores(all_per_res_scores, out_dir='')</code>","text":"<p>Write out multiple ChimeraX compatible visualisation scripts for the per residue scores, one script for each model used.</p> <p>Parameters:</p> Name Type Description Default <code>all_per_res_scores</code> <code>dict</code> <p>Nested dictionary, the outer layer keys are the model names/methods used. The inner layer is a dict with keys being each residue and values the per residue score.</p> required <code>out_dir</code> <code>str</code> <p>Folder to save outputs to, if none given, saved to current directory.</p> <code>''</code> Source code in <code>key_interactions_finder/chimerax_projections.py</code> <pre><code>def project_multiple_per_res_scores(all_per_res_scores: dict, out_dir: str = \"\") -&gt; None:\n    \"\"\"\n    Write out multiple ChimeraX compatible visualisation scripts for\n    the per residue scores, one script for each model used.\n\n    Parameters\n    ----------\n\n    all_per_res_scores : dict\n        Nested dictionary, the outer layer keys are the model names/methods used.\n        The inner layer is a dict with keys being each residue and\n        values the per residue score.\n\n    out_dir : str\n        Folder to save outputs to, if none given, saved to current directory.\n    \"\"\"\n    for model_name, model_scores in all_per_res_scores.items():\n        project_chimerax_per_res_scores(per_res_scores=model_scores, model_name=str(model_name), out_dir=out_dir)\n</code></pre>"},{"location":"contact_identification/","title":"Contact Identification","text":"<p>Responsible for identifying all non-covalent interactions in a trajectory.</p> <p>This bypasses the need for the install of PyContact and reproduces the scoring function used by PyContact.</p> <p>Output will be a csv file with each column an interaction pair. Each column has the following format: [residue1][residue2] [interaction type]</p> <p>Where \"residue1\" and \"residue2\" are the names and residue numbers of the pair. and \"interaction type\" is one of: \"Hbond\" - Hydrogen bond \"Saltbr\" - Salt Bridge \"Hydrophobic\" - VdW interaction between two hydrophobic residues. \"VdW\" - Unspecified VdW interaction.</p>"},{"location":"contact_identification/#key_interactions_finder.contact_identification.calculate_contacts","title":"<code>calculate_contacts(parm_file, traj_file, out_file, first_res=None, last_res=None, report_timings=True)</code>","text":"<p>Identify all non-covalent interactions present in the simulation and save the output. Output has each non-covalent interaction as a column each column has the following information: [residue1][residue2] [interaction type]</p> <p>Parameters:</p> Name Type Description Default <code>parm_file</code> <code>str</code> <p>The file path to your topology file. All MDAnalysis allowed topologies can be used. Please do not use a PDB file for this, use something with charge information. This is important for the hydrogen bonding part of the calculation to work.</p> required <code>traj_file</code> <code>str</code> <p>The file path to your trajectory file. All MDAnalysis allowed trajectory file types can be used.</p> required <code>out_file</code> <code>str</code> <p>Path to write the csv output file too.</p> required <code>first_res</code> <code>Optional[int]</code> <p>First residue to analyse, useful if you want to break the analysis into blocks. If not provided, the first residue in the trajectory will be used.</p> <code>None</code> <code>last_res</code> <code>Optional[int]</code> <p>Last residue to analyse, useful if you want to break the analysis into blocks. If not provided, the last residue in the trajectory will be used.</p> <code>None</code> <code>report_timings</code> <code>bool</code> <p>Choose whether to print to the console how long the job took to run. Optional, default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> <p>Output written to file. Optional timings printed to the console.</p> Source code in <code>key_interactions_finder/contact_identification.py</code> <pre><code>def calculate_contacts(\n    parm_file: str,\n    traj_file: str,\n    out_file: str,\n    first_res: Optional[int] = None,\n    last_res: Optional[int] = None,\n    report_timings: bool = True,\n) -&gt; None:\n    \"\"\"\n\n    Identify all non-covalent interactions present in the simulation and save the output.\n    Output has each non-covalent interaction as a column\n    each column has the following information: [residue1] [residue2] [interaction type]\n\n    Parameters\n    ----------\n\n    parm_file: str\n        The file path to your topology file. All MDAnalysis allowed topologies can be used.\n        Please do not use a PDB file for this, use something with charge information.\n        This is important for the hydrogen bonding part of the calculation to work.\n\n    traj_file: str\n        The file path to your trajectory file.\n        All MDAnalysis allowed trajectory file types can be used.\n\n    out_file: str\n        Path to write the csv output file too.\n\n    first_res: Optional[int]\n        First residue to analyse, useful if you want to break the analysis into blocks.\n        If not provided, the first residue in the trajectory will be used.\n\n    last_res: Optional[int]\n        Last residue to analyse, useful if you want to break the analysis into blocks.\n        If not provided, the last residue in the trajectory will be used.\n\n    report_timings: bool = True\n        Choose whether to print to the console how long the job took to run.\n        Optional, default is True.\n\n    Returns\n    -------\n\n    None\n        Output written to file. Optional timings printed to the console.\n    \"\"\"\n    if report_timings:\n        start_time = time.monotonic()\n\n    universe = Universe(parm_file, traj_file)\n\n    if first_res is None:\n        first_res = 1\n\n    biggest_protein_res = max(universe.select_atoms(\"name CA\").resids)\n    if last_res is None:\n        last_res = biggest_protein_res\n\n    if last_res &gt; biggest_protein_res:\n        warning_message = f\"\"\"\n            You stated your last residue was residue number: {last_res}.\n            I found the last protein residue to be: {biggest_protein_res}. \\n\n            This program is primarily designed to analyse protein-protein interactions, but can be used\n            on ligands, or even water molecules \\n\n            If the difference between the {last_res} and {biggest_protein_res} is very large, then check you have not\n            included all water molecules in your calculation. Doing so will make this calculation a lot slower. \\n\n\n            If the difference is small, you can safely ignore this warning message as it is probably the case you just\n            have a ligand or two in your input file.\n        \"\"\"\n        warnings.warn(warning_message, stacklevel=2)\n\n        biggest_res = last_res\n    else:\n        biggest_res = biggest_protein_res\n\n    trajectory_length = len(universe.trajectory)\n    trajectory_of_zeros = np.zeros(trajectory_length)\n\n    all_heavy_atoms_sele = \"not name H* and resid 1\" + \"-\" + str(biggest_res)\n    all_heavy_atoms = universe.select_atoms(all_heavy_atoms_sele)\n    residue_names = [names.capitalize() for names in list(universe.residues.resnames)]\n\n    # determine which residue each heavy atom belongs to.\n    residue_ranges = {}\n    for res_numb in range(1, biggest_res + 1):\n        residue_range = np.where(all_heavy_atoms.atoms.resids == res_numb)\n        residue_ranges[res_numb] = residue_range\n\n    print(\"setup complete, analysing contacts now...\")\n    hbond_pairs = _determine_hbond_pairs(universe=universe)\n\n    # Now go through each frame.\n    all_contact_scores = {}\n    for idx, _ in enumerate(universe.trajectory):  # each step is new frame.\n        # calculate all heavy atom distances for this trajectory\n        heavy_atom_dists = distances.distance_array(\n            all_heavy_atoms.positions,\n            all_heavy_atoms.positions,\n        )\n\n        for res1 in range(first_res, last_res + 1):\n            res_dists = heavy_atom_dists[residue_ranges[res1]]\n\n            # +3 here as neighbouring residues not interesting.\n            for res2 in range(res1 + 3, biggest_res + 1):\n                res_res_dists = res_dists[:, residue_ranges[res2]]\n\n                # score would be 0 if true.\n                if res_res_dists.min() &gt; MAX_HEAVY_DIST:\n                    continue\n\n                contact_score = _score_residue_contact(res_res_dists)\n                if (res1, res2) not in all_contact_scores:\n                    # create empty array of size trajectory for it...\n                    all_contact_scores[(res1, res2)] = trajectory_of_zeros.copy()\n                # now update score for this frame.\n                all_contact_scores[(res1, res2)][idx] = contact_score\n\n    contact_labels_scores = {}\n    for res_pair, contact_scores in all_contact_scores.items():\n        # save contact only if non-negligible interaction present\n        avg_contact_score = sum(contact_scores) / trajectory_length\n        if avg_contact_score &lt; 0.1:\n            continue\n\n        res1, res2 = res_pair\n        # -1 as 0 indexed...\n        res1_name = residue_names[res1 - 1]\n        res2_name = residue_names[res2 - 1]\n\n        interaction_type = _determine_interaction_type(\n            res1_id=res1,\n            res2_id=res2,\n            hbond_pairs=hbond_pairs,\n            universe=universe,\n        )\n\n        contact_label = str(res1) + res1_name + \" \" + str(res2) + res2_name + \" \" + interaction_type\n        contact_labels_scores.update({contact_label: contact_scores})\n\n    # reorders column names, to be like the old format.\n    sorted_dict = dict(\n        sorted(\n            contact_labels_scores.items(),\n            key=lambda item: (int(\"\".join(filter(str.isdigit, item[0].split(\":\")[0])))),\n        )\n    )\n\n    df_contact_scores = pd.DataFrame(sorted_dict)\n    df_contact_scores.to_csv(out_file, index=False)\n\n    if report_timings:\n        end_time = time.monotonic()\n        delta_time = timedelta(seconds=end_time - start_time)\n        print(f\"Time taken: {delta_time}\")\n</code></pre>"},{"location":"data_preperation/","title":"Data Preperation","text":"<p>Takes a processed PyContact feature set and prepares the dataset for either supervised/unsupervised learning.</p> <p>Main responsibilities: 1. Add target variable data to supervised learning datasets. 2. Offer several filtering methods for the PyContact features.</p> <p>There are 2 classes for end user usage:</p> <ol> <li> <p>SupervisedFeatureData     For supervised datasets where (there is a target variable)</p> </li> <li> <p>UnsupervisedFeatureData     For unsupervised datasets where (no target variable)</p> </li> </ol> <p>These classes both inherit from the class \"_FeatureData\", which abstracts as much as their shared behaviour as possible.</p>"},{"location":"data_preperation/#key_interactions_finder.data_preperation.SupervisedFeatureData","title":"<code>SupervisedFeatureData</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureData</code></p> <p>FeatureData Class for datasets with classification data.</p> <p>Attributes:</p> Name Type Description <code>input_df</code> <code>DataFrame</code> <p>Dataframe of PyContact features to process.</p> <code>is_classification</code> <code>bool</code> <p>Select True IF the target variable is a classifications(discrete data). Select False IF the target variable is a regression (continous data).</p> <code>target_file</code> <code>str</code> <p>String for the path to the target variable file.</p> <code>header_present</code> <code>bool</code> <p>True or False, does the target_file have a header. Default is True.</p> <code>df_processed</code> <code>DataFrame</code> <p>Dataframe generated after merging feature and classifcation data together but before any filtering has been performed.</p> <code>df_filtered</code> <code>DataFrame</code> <p>Dataframe generated after filtering. Each time a filtering method is applied, this dataset is updated so all filtering method previously performed are preserved.</p> <p>Methods:</p> Name Description <code>filter_by_occupancy</code> <p>Filter features such that only features with %occupancy &gt;= the min_occupancy are kept.</p> <code>filter_by_interaction_type</code> <p>Filter features/interactions to use by their type (e.g. hbond or vdws...)</p> <code>filter_by_main_or_side_chain</code> <p>Filter features to only certain combinations of main and side chain interactions.</p> <code>filter_by_avg_strength</code> <p>Filter features/interactions to use by their average strength.</p> <code>reset_filtering</code> <p>Reset the filtered dataframe back to its original form.</p> <code>filter_by_occupancy_by_class</code> <p>Special alternative to the the standard filter features by occupancy method. %occupancy is determined for each class (as opposed to whole dataset), meaning only observations from 1 class have to meet the cut-off to keep the feature. Only avaible to datasets with a categorical target variable (classification).</p> Source code in <code>key_interactions_finder/data_preperation.py</code> <pre><code>@dataclass\nclass SupervisedFeatureData(_FeatureData):\n    \"\"\"\n    FeatureData Class for datasets with classification data.\n\n    Attributes\n    ----------\n\n    input_df : pd.DataFrame\n        Dataframe of PyContact features to process.\n\n    is_classification : bool\n        Select True IF the target variable is a classifications(discrete data).\n        Select False IF the target variable is a regression (continous data).\n\n    target_file : str\n        String for the path to the target variable file.\n\n    header_present : bool\n        True or False, does the target_file have a header.\n        Default is True.\n\n    df_processed : pd.DataFrame\n        Dataframe generated after merging feature and classifcation data together\n        but before any filtering has been performed.\n\n    df_filtered : pd.DataFrame\n        Dataframe generated after filtering. Each time a filtering method is applied, this\n        dataset is updated so all filtering method previously performed are preserved.\n\n    Methods\n    -------\n\n    filter_by_occupancy(min_occupancy)\n        Filter features such that only features with %occupancy &gt;= the min_occupancy are kept.\n\n    filter_by_interaction_type(interaction_types_included)\n        Filter features/interactions to use by their type (e.g. hbond or vdws...)\n\n    filter_by_main_or_side_chain(main_side_chain_types_included)\n        Filter features to only certain combinations of main and side chain interactions.\n\n    filter_by_avg_strength(average_strength_cut_off)\n        Filter features/interactions to use by their average strength.\n\n    reset_filtering()\n        Reset the filtered dataframe back to its original form.\n\n    filter_by_occupancy_by_class(min_occupancy)\n        Special alternative to the the standard filter features by occupancy method.\n        %occupancy is determined for each class (as opposed to whole dataset),\n        meaning only observations from 1 class have to meet the cut-off to keep the feature.\n        Only avaible to datasets with a categorical target variable (classification).\n    \"\"\"\n\n    # Others are defined in parent class.\n    is_classification: bool\n    target_file: str\n    header_present: bool = True\n\n    def __post_init__(self):\n        \"\"\"Merge target data to the dataframe, make an empty df for df_filtered.\"\"\"\n        if self.header_present:\n            df_class = pd.read_csv(self.target_file)\n        else:\n            df_class = pd.read_csv(self.target_file, header=None)\n\n        df_class = df_class.set_axis([\"Target\"], axis=1)\n\n        if len(df_class) == len(self.input_df):\n            self.df_processed = pd.concat([df_class, self.input_df], axis=1)\n        else:\n            exception_message = (\n                f\"Number of rows for target variables data: {len(df_class)} \\n\"\n                + f\"Number of rows for PyContact data: {len(self.input_df)} \\n\"\n                + \"The length of your target variables file doesn't match the \"\n                + \"length of your features file. If the difference is 1, \"\n                + \"check if you set the 'header_present' keyword correctly.\"\n            )\n            raise ValueError(exception_message)\n\n        # Empty for now until any filtering is performed\n        self.df_filtered = pd.DataFrame()\n\n        print(\"Your PyContact features and target variable have been succesufully merged.\")\n        print(\"You can access this dataset through the class attribute: '.df_processed'.\")\n\n    def filter_by_occupancy_by_class(self, min_occupancy: float) -&gt; pd.DataFrame:\n        \"\"\"\n        Special alternative to the standard filter features by occupancy method.\n        As in the standard method, only features with %occupancy &gt;= the min_occupancy are kept.\n        (%occupancy is the % of frames that have a non-zero interaction value).\n\n        However, in this approach, %occupancy is determined for each class, meaning only\n        observations from 1 class have to meet the cut-off to keep the feature.\n\n        Only available to datasets with classification (not regression) target data.\n\n        Parameters\n        ----------\n\n        min_occupancy : float\n            Minimum %occupancy that a feature must have to be retained.\n\n        Returns\n        -------\n\n        pd.DataFrame\n            Filtered dataframe.\n        \"\"\"\n        if not self.is_classification:\n            error_message = (\n                \"Only datasets with discrete data (i.e. for classification) can use this method. \"\n                + \"You specified your target data was continous (i.e. for regression).\"\n                + \"You are likely after the method: filter_by_occupancy(min_occupancy) instead.\"\n            )\n            raise TypeError(error_message)\n\n        keep_cols = [\"Target\"]  # always want \"Target\" present...\n        try:\n            for class_label in list(self.df_filtered[\"Target\"].unique()):\n                df_single_class = self.df_filtered[(self.df_filtered[\"Target\"] == class_label)]\n                keep_cols_single_class = list(\n                    (df_single_class.loc[:, (df_single_class != 0).mean() &gt; (min_occupancy / 100)]).columns\n                )\n                keep_cols.extend(keep_cols_single_class)\n\n            self.df_filtered = self.df_filtered[list(sorted(set(keep_cols), reverse=True))]\n\n        except KeyError:  # if no other filtering has been performed yet, follow this path.\n            for class_label in list(self.df_processed[\"Target\"].unique()):\n                df_single_class = self.df_processed[(self.df_processed[\"Target\"] == class_label)]\n                keep_cols_single_class = list(\n                    (df_single_class.loc[:, (df_single_class != 0).mean() &gt; (min_occupancy / 100)]).columns\n                )\n                keep_cols.extend(keep_cols_single_class)\n\n            self.df_filtered = self.df_processed[list(sorted(set(keep_cols), reverse=True))]\n\n        return self.df_filtered\n</code></pre>"},{"location":"data_preperation/#key_interactions_finder.data_preperation.SupervisedFeatureData.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Merge target data to the dataframe, make an empty df for df_filtered.</p> Source code in <code>key_interactions_finder/data_preperation.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Merge target data to the dataframe, make an empty df for df_filtered.\"\"\"\n    if self.header_present:\n        df_class = pd.read_csv(self.target_file)\n    else:\n        df_class = pd.read_csv(self.target_file, header=None)\n\n    df_class = df_class.set_axis([\"Target\"], axis=1)\n\n    if len(df_class) == len(self.input_df):\n        self.df_processed = pd.concat([df_class, self.input_df], axis=1)\n    else:\n        exception_message = (\n            f\"Number of rows for target variables data: {len(df_class)} \\n\"\n            + f\"Number of rows for PyContact data: {len(self.input_df)} \\n\"\n            + \"The length of your target variables file doesn't match the \"\n            + \"length of your features file. If the difference is 1, \"\n            + \"check if you set the 'header_present' keyword correctly.\"\n        )\n        raise ValueError(exception_message)\n\n    # Empty for now until any filtering is performed\n    self.df_filtered = pd.DataFrame()\n\n    print(\"Your PyContact features and target variable have been succesufully merged.\")\n    print(\"You can access this dataset through the class attribute: '.df_processed'.\")\n</code></pre>"},{"location":"data_preperation/#key_interactions_finder.data_preperation.SupervisedFeatureData.filter_by_occupancy_by_class","title":"<code>filter_by_occupancy_by_class(min_occupancy)</code>","text":"<p>Special alternative to the standard filter features by occupancy method. As in the standard method, only features with %occupancy &gt;= the min_occupancy are kept. (%occupancy is the % of frames that have a non-zero interaction value).</p> <p>However, in this approach, %occupancy is determined for each class, meaning only observations from 1 class have to meet the cut-off to keep the feature.</p> <p>Only available to datasets with classification (not regression) target data.</p> <p>Parameters:</p> Name Type Description Default <code>min_occupancy</code> <code>float</code> <p>Minimum %occupancy that a feature must have to be retained.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Filtered dataframe.</p> Source code in <code>key_interactions_finder/data_preperation.py</code> <pre><code>def filter_by_occupancy_by_class(self, min_occupancy: float) -&gt; pd.DataFrame:\n    \"\"\"\n    Special alternative to the standard filter features by occupancy method.\n    As in the standard method, only features with %occupancy &gt;= the min_occupancy are kept.\n    (%occupancy is the % of frames that have a non-zero interaction value).\n\n    However, in this approach, %occupancy is determined for each class, meaning only\n    observations from 1 class have to meet the cut-off to keep the feature.\n\n    Only available to datasets with classification (not regression) target data.\n\n    Parameters\n    ----------\n\n    min_occupancy : float\n        Minimum %occupancy that a feature must have to be retained.\n\n    Returns\n    -------\n\n    pd.DataFrame\n        Filtered dataframe.\n    \"\"\"\n    if not self.is_classification:\n        error_message = (\n            \"Only datasets with discrete data (i.e. for classification) can use this method. \"\n            + \"You specified your target data was continous (i.e. for regression).\"\n            + \"You are likely after the method: filter_by_occupancy(min_occupancy) instead.\"\n        )\n        raise TypeError(error_message)\n\n    keep_cols = [\"Target\"]  # always want \"Target\" present...\n    try:\n        for class_label in list(self.df_filtered[\"Target\"].unique()):\n            df_single_class = self.df_filtered[(self.df_filtered[\"Target\"] == class_label)]\n            keep_cols_single_class = list(\n                (df_single_class.loc[:, (df_single_class != 0).mean() &gt; (min_occupancy / 100)]).columns\n            )\n            keep_cols.extend(keep_cols_single_class)\n\n        self.df_filtered = self.df_filtered[list(sorted(set(keep_cols), reverse=True))]\n\n    except KeyError:  # if no other filtering has been performed yet, follow this path.\n        for class_label in list(self.df_processed[\"Target\"].unique()):\n            df_single_class = self.df_processed[(self.df_processed[\"Target\"] == class_label)]\n            keep_cols_single_class = list(\n                (df_single_class.loc[:, (df_single_class != 0).mean() &gt; (min_occupancy / 100)]).columns\n            )\n            keep_cols.extend(keep_cols_single_class)\n\n        self.df_filtered = self.df_processed[list(sorted(set(keep_cols), reverse=True))]\n\n    return self.df_filtered\n</code></pre>"},{"location":"data_preperation/#key_interactions_finder.data_preperation.UnsupervisedFeatureData","title":"<code>UnsupervisedFeatureData</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_FeatureData</code></p> <p>FeatureData Class for datasets without a target varaible.</p> <p>Attributes:</p> Name Type Description <code>input_df</code> <code>DataFrame</code> <p>Dataframe of PyContact features to process.</p> <code>df_processed</code> <code>DataFrame</code> <p>Dataframe generated after class initialisation.</p> <code>df_filtered</code> <code>DataFrame</code> <p>Dataframe generated after filtering. If multiple filtering methods are used this is repeatedly updated, (so all filtering method performed on it are preserved).</p> <p>Methods:</p> Name Description <code>filter_by_occupancy</code> <p>Filter features such that only features with %occupancy &gt;= the min_occupancy are kept.</p> <code>filter_by_interaction_type</code> <p>Filter features/interactions to use by their type (e.g. hbond or vdws...)</p> <code>filter_by_main_or_side_chain</code> <p>Filter features to only certain combinations of main and side chain interactions.</p> <code>filter_by_avg_strength</code> <p>Filter features/interactions to use by their average strength.</p> <code>reset_filtering</code> <p>Reset the filtered dataframe back to its original form.</p> Source code in <code>key_interactions_finder/data_preperation.py</code> <pre><code>@dataclass\nclass UnsupervisedFeatureData(_FeatureData):\n    \"\"\"\n    FeatureData Class for datasets without a target varaible.\n\n    Attributes\n    ----------\n\n    input_df : pd.DataFrame\n        Dataframe of PyContact features to process.\n\n    df_processed : pd.DataFrame\n        Dataframe generated after class initialisation.\n\n    df_filtered : pd.DataFrame\n        Dataframe generated after filtering. If multiple filtering methods are used\n        this is repeatedly updated, (so all filtering method performed on it are preserved).\n\n    Methods\n    -------\n\n    filter_by_occupancy(min_occupancy)\n        Filter features such that only features with %occupancy &gt;= the min_occupancy are kept.\n\n    filter_by_interaction_type(interaction_types_included)\n        Filter features/interactions to use by their type (e.g. hbond or vdws...)\n\n    filter_by_main_or_side_chain(main_side_chain_types_included)\n        Filter features to only certain combinations of main and side chain interactions.\n\n    filter_by_avg_strength(average_strength_cut_off)\n        Filter features/interactions to use by their average strength.\n\n    reset_filtering()\n        Reset the filtered dataframe back to its original form.\n    \"\"\"\n\n    def __post_init__(self):\n        \"\"\"Initialise an empty dataframe so dataclass can be printed.\"\"\"\n        self.df_filtered = pd.DataFrame()\n\n        # A little hacky, but doing this unites the supervised + unsuperivsed methods.\n        # Save a lots of code duplication...\n        self.df_processed = self.input_df\n</code></pre>"},{"location":"data_preperation/#key_interactions_finder.data_preperation.UnsupervisedFeatureData.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Initialise an empty dataframe so dataclass can be printed.</p> Source code in <code>key_interactions_finder/data_preperation.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialise an empty dataframe so dataclass can be printed.\"\"\"\n    self.df_filtered = pd.DataFrame()\n\n    # A little hacky, but doing this unites the supervised + unsuperivsed methods.\n    # Save a lots of code duplication...\n    self.df_processed = self.input_df\n</code></pre>"},{"location":"model_building/","title":"Machine Learning","text":"<p>Module to prepare and run machine learning (ML) in either a supervised or unsupervised fashion.</p> <p>3 Classes for end user usage:</p> <ol> <li> <p>ClassificationModel     For building ML models with categorical target data (classification).</p> </li> <li> <p>RegressionModel     For building ML models with continious target data (regression).</p> </li> <li> <p>UnsupervisedModel     For building ML models for datasets without labels.     Limited support for this module at present.</p> </li> </ol> <p>These classes inherit first from an abstract base class called \"_MachineLearnModel\". which sets a basic outline for all 3 classes above.</p> <p>Classes 1 and 2 then inherit from a parent class called \"_SupervisedRunner\" which abstracts as much as their shared behaviour as possible.</p>"},{"location":"model_building/#key_interactions_finder.model_building.ClassificationModel","title":"<code>ClassificationModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_SupervisedRunner</code></p> <p>Class to construct supervised machine learning models when the target class is categorical (aka classification).</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>DataFrame</code> <p>Input dataset.</p> <code>classes_to_use</code> <code>list</code> <p>Names of the classes to train the model on. Can be left empty if you want to use all the classes you currently have. Default = [] (use all classes in the Target column.)</p> <code>models_to_use</code> <code>list</code> <p>List of machine learning models/algorithims to use. Default = [\"CatBoost\"]</p> <code>evaluation_split_ratio</code> <code>float</code> <p>Ratio of data that should be used to make the evaluation test set. The rest of the data will be used for the training/hyper-param tuning. Default = 0.15</p> <code>scaling_method</code> <code>str</code> <p>How to scale the dataset prior to machine learning. Options are \"min_max\" (scikit-learn's MinMaxScaler) or \"standard_scaling\" (scikit-learn's StandardScaler). Default = \"min_max\"</p> <code>out_dir</code> <code>str</code> <p>Directory path to store results files to. Default = \"\"</p> <code>cross_validation_splits</code> <code>int</code> <p>Number of splits in the cross validation, (the \"k\" in k-fold cross validation). Default = 5</p> <code>cross_validation_repeats</code> <code>int</code> <p>Number of repeats for the k-fold cross validation to perform. Default = 3</p> <code>search_approach</code> <code>str</code> <p>Define how extensive the grid search protocol should be for the models. Options are: \"none\", \"quick\", \"moderate\", \"exhaustive\" or \"custom\". Default = \"quick\"</p> <code>use_class_weights</code> <code>bool</code> <p>Choose to weight each class according to the number of observations. (This can be used in the case of an imbalanced dataset.) The weights that will be used are the inverse of the class distribution so each class has effective weight 1 at the end. Default = False</p> <code>all_model_params</code> <code>dict</code> <p>Nested dictionary of model parameters that can be read directly into Scikit-learn's implementation of grid search cv.</p> <code>cross_validation_approach</code> <code>RepeatedStratifiedKFold</code> <p>Instance of scikit-learn's RepeatedStratifiedKFold class for model building.</p> <code>feat_names</code> <code>ndarray</code> <p>All feature names/labels.</p> <code>ml_datasets</code> <code>dict</code> <p>Nested dictionary containing the training and testing data (both features and classes) needed to run the model building.</p> <code>label_encoder</code> <code>LabelEncoder</code> <p>Instance of sci-kit learn's label encoder to encode the target classes. Required for the XGBoost model.</p> <code>ml_models</code> <code>dict</code> <p>Keys are the model name/method and values are the instance of the built model.</p> <p>Methods:</p> Name Description <code>describe_ml_planned</code> <p>Prints a summary of what machine learning protocol has been selected.</p> <code>build_models</code> <p>Runs the machine learning and summarizes the results.</p> <code>evaluate_models</code> <p>Evaluates each ML model's performance on the validation data set and provides the user with a summary of the results.</p> <code>generate_confusion_matrix</code> <p>For each ml model used, determine the confusion matrix from the validation dataset.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>@dataclass\nclass ClassificationModel(_SupervisedRunner):\n    \"\"\"\n    Class to construct supervised machine learning models when the target class\n    is categorical (aka classification).\n\n    Attributes\n    ----------\n\n    dataset : pd.DataFrame\n        Input dataset.\n\n    classes_to_use : list\n        Names of the classes to train the model on.\n        Can be left empty if you want to use all the classes you currently have.\n        Default = [] (use all classes in the Target column.)\n\n    models_to_use : list\n        List of machine learning models/algorithims to use.\n        Default = [\"CatBoost\"]\n\n    evaluation_split_ratio : float\n        Ratio of data that should be used to make the evaluation test set.\n        The rest of the data will be used for the training/hyper-param tuning.\n        Default = 0.15\n\n    scaling_method : str\n        How to scale the dataset prior to machine learning.\n        Options are \"min_max\" (scikit-learn's MinMaxScaler)\n        or \"standard_scaling\" (scikit-learn's StandardScaler).\n        Default = \"min_max\"\n\n    out_dir : str\n        Directory path to store results files to.\n        Default = \"\"\n\n    cross_validation_splits : int\n        Number of splits in the cross validation, (the \"k\" in k-fold cross validation).\n        Default = 5\n\n    cross_validation_repeats : int\n        Number of repeats for the k-fold cross validation to perform.\n        Default = 3\n\n    search_approach : str\n        Define how extensive the grid search protocol should be for the models.\n        Options are: \"none\", \"quick\", \"moderate\", \"exhaustive\" or \"custom\".\n        Default = \"quick\"\n\n    use_class_weights : bool\n        Choose to weight each class according to the number of observations.\n        (This can be used in the case of an imbalanced dataset.)\n        The weights that will be used are the inverse of the class distribution so\n        each class has effective weight 1 at the end.\n        Default = False\n\n    all_model_params : dict\n        Nested dictionary of model parameters that can be read directly into\n        Scikit-learn's implementation of grid search cv.\n\n    cross_validation_approach : RepeatedStratifiedKFold\n        Instance of scikit-learn's RepeatedStratifiedKFold class for model building.\n\n    feat_names : np.ndarray\n        All feature names/labels.\n\n    ml_datasets : dict\n        Nested dictionary containing the training and testing data (both features and\n        classes) needed to run the model building.\n\n    label_encoder : LabelEncoder\n        Instance of sci-kit learn's label encoder to encode the target classes.\n        Required for the XGBoost model.\n\n    ml_models : dict\n        Keys are the model name/method and values are the instance of the\n        built model.\n\n    Methods\n    -------\n\n    describe_ml_planned()\n        Prints a summary of what machine learning protocol has been selected.\n\n    build_models(save_models)\n        Runs the machine learning and summarizes the results.\n\n    evaluate_models()\n        Evaluates each ML model's performance on the validation data set\n        and provides the user with a summary of the results.\n\n    generate_confusion_matrix()\n        For each ml model used, determine the confusion matrix from the validation dataset.\n    \"\"\"\n\n    # Only non-shared parameters between classificaiton and regression.\n    label_encoder: LabelEncoder = LabelEncoder()\n    classes_to_use: list = field(default_factory=[])\n    use_class_weights: bool = False\n\n    def __post_init__(self):\n        \"\"\"Setup the provided dataset and params for ML.\"\"\"\n        self.out_dir = _prep_out_dir(self.out_dir)\n\n        # not populated till build_models method called later.\n        self.all_model_params = {}\n        self.ml_models = {}\n\n        # Filter to only include desired classes.\n        if len(self.classes_to_use) != 0:\n            self.dataset = self.dataset[self.dataset[\"Target\"].isin(self.classes_to_use)]\n\n        # Train-test splitting and scaling.\n        df_features = self.dataset.drop(\"Target\", axis=1)\n        x_array = df_features.to_numpy()\n        self.feat_names = df_features.columns.values\n\n        # label encode the target - for XGBOOST compatability.\n        self.label_encoder = LabelEncoder()\n        y_classes = self.label_encoder.fit_transform(self.dataset.Target.values)\n\n        x_array_train, x_array_eval, y_train, y_eval = train_test_split(\n            x_array, y_classes, test_size=self.evaluation_split_ratio\n        )\n\n        train_data_scaled, eval_data_scaled = self._supervised_scale_features(\n            scaling_method=self.scaling_method, x_array_train=x_array_train, x_array_eval=x_array_eval\n        )\n\n        self.ml_datasets = {}\n        self.ml_datasets[\"train_data_scaled\"] = train_data_scaled\n        self.ml_datasets[\"eval_data_scaled\"] = eval_data_scaled\n        self.ml_datasets[\"y_train\"] = y_train\n        self.ml_datasets[\"y_eval\"] = y_eval\n\n        if self.use_class_weights:\n            # For CatBoostClassifier\n            classes = np.unique(y_train)\n            weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n            class_weights = dict(zip(classes, weights, strict=True))\n\n            # For XGBClassifier, I need the total number of examples in the majority class\n            # divided by the total number of examples in the minority class.\n            majority = self.dataset[\"Target\"].value_counts()[0]\n            minority = self.dataset[\"Target\"].value_counts()[1]\n            scaled_weight = round((majority / minority), 2)\n\n            self.available_models = {\n                \"CatBoost\": {\n                    \"model\": CatBoostClassifier(class_weights=class_weights, logging_level=\"Silent\"),\n                    \"params\": {},\n                },\n                \"XGBoost\": {\n                    \"model\": XGBClassifier(eval_metric=\"logloss\", scale_pos_weight=scaled_weight),\n                    \"params\": {},\n                },\n                \"Random_Forest\": {\"model\": RandomForestClassifier(class_weight=\"balanced\"), \"params\": {}},\n            }\n\n            print(\"Class weights have now been added to your dataset.\")\n            print(f\"The class imbalance in your dataset was: 1 : {1 / scaled_weight:.2f}\")\n\n        else:\n            self.available_models = {\n                \"CatBoost\": {\"model\": CatBoostClassifier(logging_level=\"Silent\"), \"params\": {}},\n                \"XGBoost\": {\"model\": XGBClassifier(eval_metric=\"logloss\"), \"params\": {}},\n                \"Random_Forest\": {\"model\": RandomForestClassifier(), \"params\": {}},\n            }\n\n        # Define ML Pipeline:\n        self.cross_validation_approach = RepeatedStratifiedKFold(\n            n_splits=self.cross_validation_splits, n_repeats=self.cross_validation_repeats\n        )\n\n        self.all_model_params = self._assign_model_params(\n            models_to_use=self.models_to_use,\n            available_models=self.available_models,\n            search_approach=self.search_approach,\n            is_classification=True,\n        )\n\n        self.describe_ml_planned()\n\n    def evaluate_models(self) -&gt; dict:\n        \"\"\"\n        Evaluates each ML model's performance on the validation data set\n        and provides the user with a summary of the results.\n\n        Returns\n        ----------\n\n        dict\n            A dictionary with keys being the model names and values being a pd.DataFrame\n            with several scoring metrics output for each model used.\n        \"\"\"\n        # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n        # yhat_decoded = self.label_encoder.inverse_transform(yhat)\n        y_eval_decoded = self.label_encoder.inverse_transform(self.ml_datasets[\"y_eval\"])\n        class_labels = (np.unique(y_eval_decoded)).tolist()\n\n        all_classification_reports = {}\n        for model_name, clf in self.ml_models.items():\n            yhat = clf.predict(self.ml_datasets[\"eval_data_scaled\"])\n            yhat_decoded = self.label_encoder.inverse_transform(yhat)\n\n            report = metrics.classification_report(y_eval_decoded, yhat_decoded, output_dict=True)\n\n            # now reformat report so dataframe friendly.\n            new_report = {}\n            for label in class_labels:\n                new_report.update({label: report[label]})\n\n            accuracy_row = {\n                \"precision\": \"N/A\",\n                \"recall\": \"N/A\",\n                \"f1-score\": report[\"accuracy\"],\n                \"support\": report[\"weighted avg\"][\"support\"],\n            }\n            new_report.update({\"accuracy\": accuracy_row})\n\n            new_report.update({\"macro avg\": report[\"macro avg\"]})\n            new_report.update({\"weighted avg\": report[\"weighted avg\"]})\n\n            df_classification_report = pd.DataFrame(new_report).transpose()\n\n            all_classification_reports.update({model_name: df_classification_report})\n\n        print(\"Returning classification reports for each model inside a single dictionary\")\n        return all_classification_reports\n\n    def generate_confusion_matrix(self) -&gt; dict:\n        \"\"\"\n        For each ml model used, determine the confusion matrix from the validation data set.\n        Returns a dictionary with model names as keys and the corresponding matrix as the values.\n\n        Returns\n        ----------\n\n        dict\n            Keys are strings of each model name. Values are the confusion matrix\n            of said model as a numpy.ndarray.\n        \"\"\"\n        confusion_matrices = {}\n        for model_name, clf in self.ml_models.items():\n            yhat = clf.predict(self.ml_datasets[\"eval_data_scaled\"])\n            y_true = self.ml_datasets[\"y_eval\"]\n\n            yhat_decoded = self.label_encoder.inverse_transform(yhat)\n            y_true_decoded = self.label_encoder.inverse_transform(y_true)\n\n            confuse_matrix = metrics.confusion_matrix(y_true_decoded, yhat_decoded)\n\n            confusion_matrices.update({model_name: confuse_matrix})\n\n        return confusion_matrices\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.ClassificationModel.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Setup the provided dataset and params for ML.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Setup the provided dataset and params for ML.\"\"\"\n    self.out_dir = _prep_out_dir(self.out_dir)\n\n    # not populated till build_models method called later.\n    self.all_model_params = {}\n    self.ml_models = {}\n\n    # Filter to only include desired classes.\n    if len(self.classes_to_use) != 0:\n        self.dataset = self.dataset[self.dataset[\"Target\"].isin(self.classes_to_use)]\n\n    # Train-test splitting and scaling.\n    df_features = self.dataset.drop(\"Target\", axis=1)\n    x_array = df_features.to_numpy()\n    self.feat_names = df_features.columns.values\n\n    # label encode the target - for XGBOOST compatability.\n    self.label_encoder = LabelEncoder()\n    y_classes = self.label_encoder.fit_transform(self.dataset.Target.values)\n\n    x_array_train, x_array_eval, y_train, y_eval = train_test_split(\n        x_array, y_classes, test_size=self.evaluation_split_ratio\n    )\n\n    train_data_scaled, eval_data_scaled = self._supervised_scale_features(\n        scaling_method=self.scaling_method, x_array_train=x_array_train, x_array_eval=x_array_eval\n    )\n\n    self.ml_datasets = {}\n    self.ml_datasets[\"train_data_scaled\"] = train_data_scaled\n    self.ml_datasets[\"eval_data_scaled\"] = eval_data_scaled\n    self.ml_datasets[\"y_train\"] = y_train\n    self.ml_datasets[\"y_eval\"] = y_eval\n\n    if self.use_class_weights:\n        # For CatBoostClassifier\n        classes = np.unique(y_train)\n        weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n        class_weights = dict(zip(classes, weights, strict=True))\n\n        # For XGBClassifier, I need the total number of examples in the majority class\n        # divided by the total number of examples in the minority class.\n        majority = self.dataset[\"Target\"].value_counts()[0]\n        minority = self.dataset[\"Target\"].value_counts()[1]\n        scaled_weight = round((majority / minority), 2)\n\n        self.available_models = {\n            \"CatBoost\": {\n                \"model\": CatBoostClassifier(class_weights=class_weights, logging_level=\"Silent\"),\n                \"params\": {},\n            },\n            \"XGBoost\": {\n                \"model\": XGBClassifier(eval_metric=\"logloss\", scale_pos_weight=scaled_weight),\n                \"params\": {},\n            },\n            \"Random_Forest\": {\"model\": RandomForestClassifier(class_weight=\"balanced\"), \"params\": {}},\n        }\n\n        print(\"Class weights have now been added to your dataset.\")\n        print(f\"The class imbalance in your dataset was: 1 : {1 / scaled_weight:.2f}\")\n\n    else:\n        self.available_models = {\n            \"CatBoost\": {\"model\": CatBoostClassifier(logging_level=\"Silent\"), \"params\": {}},\n            \"XGBoost\": {\"model\": XGBClassifier(eval_metric=\"logloss\"), \"params\": {}},\n            \"Random_Forest\": {\"model\": RandomForestClassifier(), \"params\": {}},\n        }\n\n    # Define ML Pipeline:\n    self.cross_validation_approach = RepeatedStratifiedKFold(\n        n_splits=self.cross_validation_splits, n_repeats=self.cross_validation_repeats\n    )\n\n    self.all_model_params = self._assign_model_params(\n        models_to_use=self.models_to_use,\n        available_models=self.available_models,\n        search_approach=self.search_approach,\n        is_classification=True,\n    )\n\n    self.describe_ml_planned()\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.ClassificationModel.evaluate_models","title":"<code>evaluate_models()</code>","text":"<p>Evaluates each ML model's performance on the validation data set and provides the user with a summary of the results.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary with keys being the model names and values being a pd.DataFrame with several scoring metrics output for each model used.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>def evaluate_models(self) -&gt; dict:\n    \"\"\"\n    Evaluates each ML model's performance on the validation data set\n    and provides the user with a summary of the results.\n\n    Returns\n    ----------\n\n    dict\n        A dictionary with keys being the model names and values being a pd.DataFrame\n        with several scoring metrics output for each model used.\n    \"\"\"\n    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n    # yhat_decoded = self.label_encoder.inverse_transform(yhat)\n    y_eval_decoded = self.label_encoder.inverse_transform(self.ml_datasets[\"y_eval\"])\n    class_labels = (np.unique(y_eval_decoded)).tolist()\n\n    all_classification_reports = {}\n    for model_name, clf in self.ml_models.items():\n        yhat = clf.predict(self.ml_datasets[\"eval_data_scaled\"])\n        yhat_decoded = self.label_encoder.inverse_transform(yhat)\n\n        report = metrics.classification_report(y_eval_decoded, yhat_decoded, output_dict=True)\n\n        # now reformat report so dataframe friendly.\n        new_report = {}\n        for label in class_labels:\n            new_report.update({label: report[label]})\n\n        accuracy_row = {\n            \"precision\": \"N/A\",\n            \"recall\": \"N/A\",\n            \"f1-score\": report[\"accuracy\"],\n            \"support\": report[\"weighted avg\"][\"support\"],\n        }\n        new_report.update({\"accuracy\": accuracy_row})\n\n        new_report.update({\"macro avg\": report[\"macro avg\"]})\n        new_report.update({\"weighted avg\": report[\"weighted avg\"]})\n\n        df_classification_report = pd.DataFrame(new_report).transpose()\n\n        all_classification_reports.update({model_name: df_classification_report})\n\n    print(\"Returning classification reports for each model inside a single dictionary\")\n    return all_classification_reports\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.ClassificationModel.generate_confusion_matrix","title":"<code>generate_confusion_matrix()</code>","text":"<p>For each ml model used, determine the confusion matrix from the validation data set. Returns a dictionary with model names as keys and the corresponding matrix as the values.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Keys are strings of each model name. Values are the confusion matrix of said model as a numpy.ndarray.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>def generate_confusion_matrix(self) -&gt; dict:\n    \"\"\"\n    For each ml model used, determine the confusion matrix from the validation data set.\n    Returns a dictionary with model names as keys and the corresponding matrix as the values.\n\n    Returns\n    ----------\n\n    dict\n        Keys are strings of each model name. Values are the confusion matrix\n        of said model as a numpy.ndarray.\n    \"\"\"\n    confusion_matrices = {}\n    for model_name, clf in self.ml_models.items():\n        yhat = clf.predict(self.ml_datasets[\"eval_data_scaled\"])\n        y_true = self.ml_datasets[\"y_eval\"]\n\n        yhat_decoded = self.label_encoder.inverse_transform(yhat)\n        y_true_decoded = self.label_encoder.inverse_transform(y_true)\n\n        confuse_matrix = metrics.confusion_matrix(y_true_decoded, yhat_decoded)\n\n        confusion_matrices.update({model_name: confuse_matrix})\n\n    return confusion_matrices\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.RegressionModel","title":"<code>RegressionModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_SupervisedRunner</code></p> <p>Class to construct supervised machine learning models when the target class is contionous (aka regression).</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>DataFrame</code> <p>Input dataset.</p> <code>models_to_use</code> <code>list</code> <p>List of machine learning models/algorithims to use. Default = [\"CatBoost\"]</p> <code>evaluation_split_ratio</code> <code>float</code> <p>Ratio of data that should be used to make the evaluation test set. The rest of the data will be used for the training/hyper-param tuning. Default = 0.15</p> <code>scaling_method</code> <code>str</code> <p>How to scale the dataset prior to machine learning. Options are \"min_max\" (scikit-learn's MinMaxScaler) or \"standard_scaling\" (scikit-learn's StandardScaler). Default = \"min_max\"</p> <code>out_dir</code> <code>str</code> <p>Directory path to store results files to. Default = \"\"</p> <code>cross_validation_splits</code> <code>int</code> <p>Number of splits in the cross validation, (the \"k\" in k-fold cross validation). Default = 5</p> <code>cross_validation_repeats</code> <code>int</code> <p>Number of repeats for the k-fold cross validation to perform. Default = 3</p> <code>search_approach</code> <code>str</code> <p>Define how extensive the grid search protocol should be for the models. Options are: \"none\", \"quick\", \"moderate\", \"exhaustive\" or \"custom\". Default = \"quick\"</p> <code>all_model_params</code> <code>dict</code> <p>Nested dictionary of model parameters that can be read directly into Scikit-learn's implementation of grid search cv.</p> <code>cross_validation_approach</code> <code>RepeatedStratifiedKFold</code> <p>Instance of scikit-learn's RepeatedStratifiedKFold class for model building.</p> <code>feat_names</code> <code>ndarray</code> <p>All feature names/labels.</p> <code>ml_datasets</code> <code>dict</code> <p>Nested dictionary containing the training and testing data (both features and classes) needed to run the model building.</p> <code>ml_models</code> <code>dict</code> <p>Keys are the model name/method and values are the instance of the built model.</p> <p>Methods:</p> Name Description <code>describe_ml_planned</code> <p>Prints a summary of what machine learning protocol has been selected.</p> <code>build_models</code> <p>Runs the machine learning and summarizes the results.</p> <code>evaluate_models</code> <p>Evaluates each ML model's performance on the validation data set and provides the user with a summary of the results.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>@dataclass\nclass RegressionModel(_SupervisedRunner):\n    \"\"\"\n    Class to construct supervised machine learning models when the target class\n    is contionous (aka regression).\n\n    Attributes\n    ----------\n\n    dataset : pd.DataFrame\n        Input dataset.\n\n    models_to_use : list\n        List of machine learning models/algorithims to use.\n        Default = [\"CatBoost\"]\n\n    evaluation_split_ratio : float\n        Ratio of data that should be used to make the evaluation test set.\n        The rest of the data will be used for the training/hyper-param tuning.\n        Default = 0.15\n\n    scaling_method : str\n        How to scale the dataset prior to machine learning.\n        Options are \"min_max\" (scikit-learn's MinMaxScaler)\n        or \"standard_scaling\" (scikit-learn's StandardScaler).\n        Default = \"min_max\"\n\n    out_dir : str\n        Directory path to store results files to.\n        Default = \"\"\n\n    cross_validation_splits : int\n        Number of splits in the cross validation, (the \"k\" in k-fold cross validation).\n        Default = 5\n\n    cross_validation_repeats : int\n        Number of repeats for the k-fold cross validation to perform.\n        Default = 3\n\n    search_approach : str\n        Define how extensive the grid search protocol should be for the models.\n        Options are: \"none\", \"quick\", \"moderate\", \"exhaustive\" or \"custom\".\n        Default = \"quick\"\n\n    all_model_params : dict\n        Nested dictionary of model parameters that can be read directly into\n        Scikit-learn's implementation of grid search cv.\n\n    cross_validation_approach : RepeatedStratifiedKFold\n        Instance of scikit-learn's RepeatedStratifiedKFold class for model building.\n\n    feat_names : np.ndarray\n        All feature names/labels.\n\n    ml_datasets : dict\n        Nested dictionary containing the training and testing data (both features and\n        classes) needed to run the model building.\n\n    ml_models : dict\n        Keys are the model name/method and values are the instance of the\n        built model.\n\n    Methods\n    -------\n\n    describe_ml_planned()\n        Prints a summary of what machine learning protocol has been selected.\n\n    build_models(save_models)\n        Runs the machine learning and summarizes the results.\n\n    evaluate_models()\n        Evaluates each ML model's performance on the validation data set\n        and provides the user with a summary of the results.\n    \"\"\"\n\n    available_models = {\n        \"CatBoost\": {\"model\": CatBoostRegressor(logging_level=\"Silent\"), \"params\": {}},\n        \"XGBoost\": {\"model\": XGBRegressor(objective=\"reg:squarederror\"), \"params\": {}},\n        \"Random_Forest\": {\"model\": RandomForestRegressor(), \"params\": {}},\n    }\n\n    # This is called at the end of the dataclass's initialization procedure.\n    def __post_init__(self):\n        \"\"\"Setup the provided dataset and params for ML.\"\"\"\n        self.out_dir = _prep_out_dir(self.out_dir)\n\n        # These are all not populated till a method is called later.\n        self.ml_models = {}\n        self.all_model_params = {}\n\n        # Train-test splitting and scaling.\n        df_features = self.dataset.drop(\"Target\", axis=1)\n        x_array = df_features.to_numpy()\n        self.feat_names = df_features.columns.values\n        y_classes = self.dataset[\"Target\"]\n        x_array_train, x_array_eval, y_train, y_eval = train_test_split(\n            x_array, y_classes, test_size=self.evaluation_split_ratio\n        )\n\n        train_data_scaled, eval_data_scaled = self._supervised_scale_features(\n            scaling_method=self.scaling_method, x_array_train=x_array_train, x_array_eval=x_array_eval\n        )\n\n        self.ml_datasets = {}\n        self.ml_datasets[\"train_data_scaled\"] = train_data_scaled\n        self.ml_datasets[\"eval_data_scaled\"] = eval_data_scaled\n        self.ml_datasets[\"y_train\"] = y_train\n        self.ml_datasets[\"y_eval\"] = y_eval\n\n        # Define ML Pipeline:\n        self.cross_validation_approach = RepeatedKFold(\n            n_splits=self.cross_validation_splits, n_repeats=self.cross_validation_repeats\n        )\n\n        self.all_model_params = self._assign_model_params(\n            models_to_use=self.models_to_use,\n            available_models=self.available_models,\n            search_approach=self.search_approach,\n            is_classification=False,\n        )\n\n        self.describe_ml_planned()\n\n    def evaluate_models(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Evaluates each ML model's performance on the validation data set\n        and provides the user with a summary of the results.\n\n        Returns\n        ----------\n\n        pd.DataFrame\n            Dataframe with each row a containing several regression metrics\n            for each ML model generated.\n        \"\"\"\n        all_regression_dfs = []\n        for model_name, clf in self.ml_models.items():\n            y_validation = self.ml_datasets[\"y_eval\"]\n            yhat = clf.predict(self.ml_datasets[\"eval_data_scaled\"])\n\n            regression_df = self._regression_metrics(model_name=model_name, y_true=y_validation, y_pred=yhat)\n\n            all_regression_dfs.append(regression_df)\n\n        return pd.concat(all_regression_dfs).reset_index(drop=True)\n\n    @staticmethod\n    def _regression_metrics(model_name: str, y_true: np.ndarray, y_pred: np.ndarray) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate several regression statistics for a given ml model.\n\n        Adapted from:\n        https://stackoverflow.com/questions/26319259/how-to-get-a-regression-summary-in-scikit-learn-like-r-does\n\n        Parameters\n        ----------\n\n        model_name : str\n            Name of the ML model.\n\n        y_true : np.ndarray\n            1D array of the actual values of the target label.\n\n        y_pred: np.ndarray\n            1D array of the predicted values of the target label.\n\n        Returns\n        ----------\n\n        pd.DataFrame\n            Contains various regression metrics for the provided model.\n        \"\"\"\n        explained_variance = np.round(metrics.explained_variance_score(y_true, y_pred), 4)\n\n        mean_absolute_error = np.round(metrics.mean_absolute_error(y_true, y_pred), 4)\n\n        mse = np.round(metrics.mean_squared_error(y_true, y_pred), 4)\n\n        rmse = np.round(np.sqrt(metrics.mean_squared_error(y_true, y_pred)), 4)\n\n        r_squared = np.round(metrics.r2_score(y_true, y_pred), 4)\n\n        try:\n            mean_squared_log_error = np.round(metrics.mean_squared_log_error(y_true, y_pred), 4)\n\n        except ValueError:\n            print(\"\"\"Mean Squared Log Error cannot be calculated as your target column contains\n                  negative numbers. Continuing with the other metrics.\"\"\")\n            mean_squared_log_error = \"N/A\"\n\n        all_metrics = [\n            [model_name, explained_variance, mean_absolute_error, mse, rmse, mean_squared_log_error, r_squared]\n        ]\n\n        column_labels = [\n            \"Model\",\n            \"Explained Variance\",\n            \"Mean Absolute Error\",\n            \"MSE\",\n            \"RMSE\",\n            \"Mean Squared Log Error\",\n            \"r squared\",\n        ]\n\n        return pd.DataFrame(all_metrics, columns=column_labels)\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.RegressionModel.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Setup the provided dataset and params for ML.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Setup the provided dataset and params for ML.\"\"\"\n    self.out_dir = _prep_out_dir(self.out_dir)\n\n    # These are all not populated till a method is called later.\n    self.ml_models = {}\n    self.all_model_params = {}\n\n    # Train-test splitting and scaling.\n    df_features = self.dataset.drop(\"Target\", axis=1)\n    x_array = df_features.to_numpy()\n    self.feat_names = df_features.columns.values\n    y_classes = self.dataset[\"Target\"]\n    x_array_train, x_array_eval, y_train, y_eval = train_test_split(\n        x_array, y_classes, test_size=self.evaluation_split_ratio\n    )\n\n    train_data_scaled, eval_data_scaled = self._supervised_scale_features(\n        scaling_method=self.scaling_method, x_array_train=x_array_train, x_array_eval=x_array_eval\n    )\n\n    self.ml_datasets = {}\n    self.ml_datasets[\"train_data_scaled\"] = train_data_scaled\n    self.ml_datasets[\"eval_data_scaled\"] = eval_data_scaled\n    self.ml_datasets[\"y_train\"] = y_train\n    self.ml_datasets[\"y_eval\"] = y_eval\n\n    # Define ML Pipeline:\n    self.cross_validation_approach = RepeatedKFold(\n        n_splits=self.cross_validation_splits, n_repeats=self.cross_validation_repeats\n    )\n\n    self.all_model_params = self._assign_model_params(\n        models_to_use=self.models_to_use,\n        available_models=self.available_models,\n        search_approach=self.search_approach,\n        is_classification=False,\n    )\n\n    self.describe_ml_planned()\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.RegressionModel.evaluate_models","title":"<code>evaluate_models()</code>","text":"<p>Evaluates each ML model's performance on the validation data set and provides the user with a summary of the results.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with each row a containing several regression metrics for each ML model generated.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>def evaluate_models(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Evaluates each ML model's performance on the validation data set\n    and provides the user with a summary of the results.\n\n    Returns\n    ----------\n\n    pd.DataFrame\n        Dataframe with each row a containing several regression metrics\n        for each ML model generated.\n    \"\"\"\n    all_regression_dfs = []\n    for model_name, clf in self.ml_models.items():\n        y_validation = self.ml_datasets[\"y_eval\"]\n        yhat = clf.predict(self.ml_datasets[\"eval_data_scaled\"])\n\n        regression_df = self._regression_metrics(model_name=model_name, y_true=y_validation, y_pred=yhat)\n\n        all_regression_dfs.append(regression_df)\n\n    return pd.concat(all_regression_dfs).reset_index(drop=True)\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.UnsupervisedModel","title":"<code>UnsupervisedModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_MachineLearnModel</code></p> <p>Class to construct machine learning models for when there is no target class available (aka, unsupervised learning).</p> <p>At present there is limited support for this, with only principal component analysis (PCA) available.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>DataFrame</code> <p>Input dataset.</p> <code>out_dir</code> <code>str</code> <p>Directory path to store results files to. Default = \"\"</p> <code>feat_names</code> <code>ndarray</code> <p>All feature names/labels.</p> <code>data_scaled</code> <code>ndarray</code> <p>Input dataset scaled with Standard scaling.</p> <code>ml_models</code> <code>dict</code> <p>Keys are the model name/method and values are the instance of the built model.</p> <p>Methods:</p> Name Description <code>describe_ml_planned</code> <p>Prints a summary of what machine learning protocol has been selected.</p> <code>build_models</code> <p>Runs the machine learning and summarizes the results.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>@dataclass\nclass UnsupervisedModel(_MachineLearnModel):\n    \"\"\"\n    Class to construct machine learning models for when there is\n    no target class available (aka, unsupervised learning).\n\n    At present there is limited support for this, with only\n    principal component analysis (PCA) available.\n\n    Attributes\n    ----------\n\n    dataset : pd.DataFrame\n        Input dataset.\n\n    out_dir : str\n        Directory path to store results files to.\n        Default = \"\"\n\n    feat_names : np.ndarray\n        All feature names/labels.\n\n    data_scaled : np.ndarray\n        Input dataset scaled with Standard scaling.\n\n    ml_models : dict\n        Keys are the model name/method and values are the instance of the\n        built model.\n\n    Methods\n    -------\n\n    describe_ml_planned()\n        Prints a summary of what machine learning protocol has been selected.\n\n    build_models(save_models)\n        Runs the machine learning and summarizes the results.\n    \"\"\"\n\n    dataset: pd.DataFrame\n    out_dir: str = \"\"\n\n    # Dynamically generated:\n    feat_names: np.ndarray = field(init=False)\n    data_scaled: np.ndarray = field(init=False)\n    ml_models: dict = field(init=False)\n\n    # This is called at the end of the dataclass's initialization procedure.\n    def __post_init__(self):\n        \"\"\"Setup the provided dataset and params for ML.\"\"\"\n        self.out_dir = _prep_out_dir(self.out_dir)\n\n        self.ml_models = {}\n\n        # Allow a user with a supervised dataset to do unsupervised learning.\n        with contextlib.suppress(KeyError):\n            self.dataset = self.dataset.drop([\"Target\"], axis=1)\n\n        self.feat_names = self.dataset.columns.values\n        data_array = self.dataset.to_numpy()\n\n        scaler = StandardScaler()\n        self.data_scaled = scaler.fit_transform(data_array)\n\n        self.describe_ml_planned()\n\n    def describe_ml_planned(self) -&gt; None:\n        \"\"\"Prints a summary of what machine learning protocol has been selected.\"\"\"\n        out_text = \"\\n\"\n        out_text += \"Below is a summary of the unsupervised machine learning you have planned. \\n\"\n\n        out_text += f\"You will use {len(self.dataset.columns)} features to build the model, with \"\n        out_text += \"all of your data will be used for training the model, \"\n        out_text += f\"which is {len(self.dataset)} observations.\\n\"\n\n        out_text += \"Currently you will use principal component analysis to get your results. \"\n        out_text += \"More methods might be added in the future. \"\n\n        out_text += \"If you're happy with the above, lets get model building!\"\n        return out_text\n\n    def build_models(self, save_models: bool = True) -&gt; None:\n        \"\"\"\n        Runs the machine learning and summarizes the results.\n\n        Parameters\n        ----------\n\n        save_models : bool\n            Whether to save the ML models made to disk.\n            Default is True.\n        \"\"\"\n        pca = PCA()\n        pca.fit(self.data_scaled)\n        self.ml_models[\"PCA\"] = pca\n        print(\"All models built.\")\n\n        if save_models:\n            temp_folder = Path(\"temporary_files\")\n            if not temp_folder.exists():\n                Path.mkdir(temp_folder)\n\n            feat_names_file = Path(temp_folder, \"feature_names.npy\")\n            np.save(feat_names_file, self.feat_names)\n\n            model_out_path = Path(temp_folder, \"PCA_Model.pickle\")\n            self._save_best_models(best_model=pca, out_path=model_out_path)\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.UnsupervisedModel.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Setup the provided dataset and params for ML.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Setup the provided dataset and params for ML.\"\"\"\n    self.out_dir = _prep_out_dir(self.out_dir)\n\n    self.ml_models = {}\n\n    # Allow a user with a supervised dataset to do unsupervised learning.\n    with contextlib.suppress(KeyError):\n        self.dataset = self.dataset.drop([\"Target\"], axis=1)\n\n    self.feat_names = self.dataset.columns.values\n    data_array = self.dataset.to_numpy()\n\n    scaler = StandardScaler()\n    self.data_scaled = scaler.fit_transform(data_array)\n\n    self.describe_ml_planned()\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.UnsupervisedModel.build_models","title":"<code>build_models(save_models=True)</code>","text":"<p>Runs the machine learning and summarizes the results.</p> <p>Parameters:</p> Name Type Description Default <code>save_models</code> <code>bool</code> <p>Whether to save the ML models made to disk. Default is True.</p> <code>True</code> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>def build_models(self, save_models: bool = True) -&gt; None:\n    \"\"\"\n    Runs the machine learning and summarizes the results.\n\n    Parameters\n    ----------\n\n    save_models : bool\n        Whether to save the ML models made to disk.\n        Default is True.\n    \"\"\"\n    pca = PCA()\n    pca.fit(self.data_scaled)\n    self.ml_models[\"PCA\"] = pca\n    print(\"All models built.\")\n\n    if save_models:\n        temp_folder = Path(\"temporary_files\")\n        if not temp_folder.exists():\n            Path.mkdir(temp_folder)\n\n        feat_names_file = Path(temp_folder, \"feature_names.npy\")\n        np.save(feat_names_file, self.feat_names)\n\n        model_out_path = Path(temp_folder, \"PCA_Model.pickle\")\n        self._save_best_models(best_model=pca, out_path=model_out_path)\n</code></pre>"},{"location":"model_building/#key_interactions_finder.model_building.UnsupervisedModel.describe_ml_planned","title":"<code>describe_ml_planned()</code>","text":"<p>Prints a summary of what machine learning protocol has been selected.</p> Source code in <code>key_interactions_finder/model_building.py</code> <pre><code>def describe_ml_planned(self) -&gt; None:\n    \"\"\"Prints a summary of what machine learning protocol has been selected.\"\"\"\n    out_text = \"\\n\"\n    out_text += \"Below is a summary of the unsupervised machine learning you have planned. \\n\"\n\n    out_text += f\"You will use {len(self.dataset.columns)} features to build the model, with \"\n    out_text += \"all of your data will be used for training the model, \"\n    out_text += f\"which is {len(self.dataset)} observations.\\n\"\n\n    out_text += \"Currently you will use principal component analysis to get your results. \"\n    out_text += \"More methods might be added in the future. \"\n\n    out_text += \"If you're happy with the above, lets get model building!\"\n    return out_text\n</code></pre>"},{"location":"network_analysis/","title":"Network Analysis","text":"<p>Perform correlation analysis on the PyContact interactions/features and outputs the results in such a way that it can easily be read into various network/correlation based analyses tools in different programs.</p> <p>There is only a single class in this module (CorrelationNetwork) as this module does not require a target variable and if you have one it does not need it.</p>"},{"location":"network_analysis/#key_interactions_finder.network_analysis.CorrelationNetwork","title":"<code>CorrelationNetwork</code>  <code>dataclass</code>","text":"<p>Handles the correlation analysis on PyContact datasets. Does not require or make use of class labels (i.e., dataset can be unsupervised).</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>DataFrame</code> <p>Input Dataframe containing all features to be studied.</p> <code>feature_corr_matrix</code> <code>DataFrame</code> <p>Correlation matrix for the dataset provided.</p> <p>Methods:</p> Name Description <code>gen_res_contact_matrix</code> <p>Generate a per residue contact map (matrix) that identifies whether two residues are in contact with each other.</p> <code>gen_res_correl_matrix</code> <p>For every residue to every other residue determine the interaction (if exists) with the strongest correlation between them and use it to build a per residue correlation matrix.</p> Source code in <code>key_interactions_finder/network_analysis.py</code> <pre><code>@dataclass\nclass CorrelationNetwork:\n    \"\"\"\n    Handles the correlation analysis on PyContact datasets.\n    Does not require or make use of class labels (i.e., dataset can be unsupervised).\n\n    Attributes\n    ----------\n\n    dataset : pd.DataFrame\n        Input Dataframe containing all features to be studied.\n\n    feature_corr_matrix : pd.DataFrame\n        Correlation matrix for the dataset provided.\n\n    Methods\n    -------\n\n    gen_res_contact_matrix(out_file)\n        Generate a per residue contact map (matrix) that identifies whether two residues\n        are in contact with each other.\n\n    gen_res_correl_matrix(out_file)\n        For every residue to every other residue determine the interaction (if exists)\n        with the strongest correlation between them and use it to build a per residue\n        correlation matrix.\n    \"\"\"\n\n    dataset: pd.DataFrame\n\n    # Generated during init.\n    feature_corr_matrix: pd.DataFrame = field(init=False)\n\n    def __post_init__(self):\n        \"\"\"Filter features and generate the full correlation matrix.\"\"\"\n        with contextlib.suppress(KeyError):\n            self.dataset = self.dataset.drop([\"Target\"], axis=1)\n\n        self.feature_corr_matrix = self.dataset.corr()\n        return self.feature_corr_matrix\n\n    def gen_res_correl_matrix(self, out_file: Optional[str] = None) -&gt; np.ndarray:\n        \"\"\"\n        For every residue to every other residue determine the interaction (if exists)\n        with the strongest correlation between them and use it to build a per residue\n        correlation matrix.\n\n        Parameters\n        ----------\n\n        out_file : Optional[str]\n            Path to save the corelation matrix to. If left empty no file is saved.\n\n        Returns\n        ----------\n\n        np.ndarray\n            A symmetrical matrix (along diagonal) of correlations between each residue.\n        \"\"\"\n        # Generate an empty correlation matrix for all residues.\n        last_residue = self._get_last_residue()\n        per_res_corr_matrix = np.zeros((last_residue, last_residue), dtype=float)\n\n        # Filter correlation matrix to only include columns with specific res number.\n        for res1 in range(1, last_residue + 1):\n            res1_regex_key = self._build_regex_strs(res_number=res1)\n            res1_matrix = self.feature_corr_matrix.filter(regex=res1_regex_key, axis=1)\n\n            # Filter matrix on other axis so matrix contains only the pairs of residues.\n            if len(res1_matrix.columns) != 0:\n                for res2 in range(res1 + 1, last_residue + 1):\n                    res2_regex_key = self._build_regex_strs(res_number=res2)\n                    res1_res2_matrix = res1_matrix.filter(regex=res2_regex_key, axis=0)\n\n                    if len(res1_res2_matrix) != 0:\n                        correls = res1_res2_matrix.to_numpy()\n\n                        try:\n                            # prevent identical interactions (== 1) being used.\n                            correls = correls[correls != 1]\n                            max_correl = max(correls.min(), correls.max(), key=abs)\n\n                            per_res_corr_matrix[(res1 - 1), (res2 - 1)] = max_correl\n                            per_res_corr_matrix[(res2 - 1), (res1 - 1)] = max_correl\n\n                        # ValueError will happen if array becomes empty\n                        # when only identical interactions were present in matrix.\n                        except ValueError:\n                            pass  # value stays at 0.\n\n        # correlation of residue to itself is 1.\n        np.fill_diagonal(per_res_corr_matrix, 1)\n\n        if out_file is not None:\n            np.savetxt(out_file, per_res_corr_matrix, delimiter=\" \", fmt=\"%.2f\")\n            print(f\"{out_file} saved to disk.\")\n\n        return per_res_corr_matrix\n\n    def gen_res_contact_matrix(self, out_file: Optional[str] = None) -&gt; np.ndarray:\n        \"\"\"\n        Generate a per residue contact map (matrix) that identifies whether two residues\n        are in contact with each other. Two residues considered in contact with one\n        another if they share an interaction (i.e. a column name).\n\n        Parameters\n        ----------\n\n        out_file : Optional[str]\n            Path to save the corelation matrix to. If left empty no file is saved.\n\n        Returns\n        ----------\n\n        np.ndarray\n            A symmetrical matrix (along diagonal) of 1s (in contact) and 0s (not in contact).\n        \"\"\"\n        # Generate empty matrix for each residue\n        last_residue = self._get_last_residue()\n        per_res_contact_matrix = np.zeros((last_residue, last_residue), dtype=int)\n\n        contact_pairs = self._get_contact_pairs()\n        for res1, res2 in contact_pairs.items():\n            per_res_contact_matrix[(res1 - 1), (res2 - 1)] = 1\n            per_res_contact_matrix[(res2 - 1), (res1 - 1)] = 1\n\n        # correlation of residue to itself is 1.\n        np.fill_diagonal(per_res_contact_matrix, 1)\n\n        if out_file is not None:\n            np.savetxt(out_file, per_res_contact_matrix, delimiter=\" \", fmt=\"%.2f\")\n            print(f\"{out_file} saved to disk.\")\n\n        return per_res_contact_matrix\n\n    def _get_residue_lists(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Given a dataframe (self.dataset) containing only PyContact features,\n        extract the residue numbers for each contact. (Helper Function.)\n\n        Returns\n        ----------\n\n        pd.DataFrame\n            1st and 2nd residue number of each contact/feature in the dataframe.\n        \"\"\"\n        res1_numbs, res2_numbs = [], []\n        for residue_pair in list(self.dataset.columns):\n            res1_info, res2_info, _ = residue_pair.split(\" \")\n            res1_numb = int(re.findall(r\"\\d+\", res1_info)[0])\n            res2_numb = int(re.findall(r\"\\d+\", res2_info)[0])\n            res1_numbs.append(res1_numb)\n            res2_numbs.append(res2_numb)\n\n        res_pairs_dict = {\"Res1\": res1_numbs, \"Res2\": res2_numbs}\n        return pd.DataFrame(res_pairs_dict)\n\n    def _get_last_residue(self) -&gt; int:\n        \"\"\"\n        Given a dataframe (self.dataset) containing only PyContact features,\n        find the last residue in the sequence.\n        (Helper function for generating the per residue matrices,\n        so one knows when to stop).\n\n        Returns\n        ----------\n\n        int\n            Largest residue number present in the dataset.\n        \"\"\"\n        df_cols = pd.DataFrame()\n        df_cols[[\"Res1\", \"Res2\"]] = self._get_residue_lists()\n        max_res1 = df_cols[\"Res1\"].max(axis=0)\n        max_res2 = df_cols[\"Res2\"].max(axis=0)\n        return max(max_res1, max_res2)\n\n    def _get_contact_pairs(self) -&gt; dict:\n        \"\"\"\n        Given a dataframe (self.dataset) containing only PyContact features,\n        extract the pairs of residue in contact with one another.\n        (Helper function to gen protein contact map).\n\n        Returns\n        ----------\n\n        dict\n            Dictionary of all interacting residue pairs.\n        \"\"\"\n        df_cols = pd.DataFrame()\n        df_cols[[\"Res1\", \"Res2\"]] = self._get_residue_lists()\n        contact_pairs = dict(zip(df_cols[\"Res1\"], df_cols[\"Res2\"], strict=True))\n        return contact_pairs\n\n    @staticmethod\n    def _build_regex_strs(res_number: int) -&gt; str:\n        \"\"\"\n        Given a residue number, return a regex string that will match only that\n        residue number in a dataframe filled with pycontact features.\n\n        This is not as simple as it first seems as easy to catch other residues.\n        E.g. If residue is 1Arg, easy to accidently catch 11Arg and so on.\n\n        Parameters\n        ----------\n\n        res_number: int\n            Residue number to create the regex string for.\n\n        Returns\n        ----------\n\n        str\n            Regex str to be used to filter a dataframe with pycontact features.\n        \"\"\"\n        # matches if target res number is 1st residue in name\n        regex_key_p1 = \"(^\" + str(res_number) + \")\" + \"([A-Za-z]{3})\" + \" \"\n\n        # matches if target res number is 2nd residue in name\n        regex_key_p2 = \" \" + str(res_number) + \"([A-Za-z]{3})\" + \" \"\n\n        return regex_key_p1 + \"|\" + regex_key_p2\n</code></pre>"},{"location":"network_analysis/#key_interactions_finder.network_analysis.CorrelationNetwork.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Filter features and generate the full correlation matrix.</p> Source code in <code>key_interactions_finder/network_analysis.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Filter features and generate the full correlation matrix.\"\"\"\n    with contextlib.suppress(KeyError):\n        self.dataset = self.dataset.drop([\"Target\"], axis=1)\n\n    self.feature_corr_matrix = self.dataset.corr()\n    return self.feature_corr_matrix\n</code></pre>"},{"location":"network_analysis/#key_interactions_finder.network_analysis.CorrelationNetwork.gen_res_contact_matrix","title":"<code>gen_res_contact_matrix(out_file=None)</code>","text":"<p>Generate a per residue contact map (matrix) that identifies whether two residues are in contact with each other. Two residues considered in contact with one another if they share an interaction (i.e. a column name).</p> <p>Parameters:</p> Name Type Description Default <code>out_file</code> <code>Optional[str]</code> <p>Path to save the corelation matrix to. If left empty no file is saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A symmetrical matrix (along diagonal) of 1s (in contact) and 0s (not in contact).</p> Source code in <code>key_interactions_finder/network_analysis.py</code> <pre><code>def gen_res_contact_matrix(self, out_file: Optional[str] = None) -&gt; np.ndarray:\n    \"\"\"\n    Generate a per residue contact map (matrix) that identifies whether two residues\n    are in contact with each other. Two residues considered in contact with one\n    another if they share an interaction (i.e. a column name).\n\n    Parameters\n    ----------\n\n    out_file : Optional[str]\n        Path to save the corelation matrix to. If left empty no file is saved.\n\n    Returns\n    ----------\n\n    np.ndarray\n        A symmetrical matrix (along diagonal) of 1s (in contact) and 0s (not in contact).\n    \"\"\"\n    # Generate empty matrix for each residue\n    last_residue = self._get_last_residue()\n    per_res_contact_matrix = np.zeros((last_residue, last_residue), dtype=int)\n\n    contact_pairs = self._get_contact_pairs()\n    for res1, res2 in contact_pairs.items():\n        per_res_contact_matrix[(res1 - 1), (res2 - 1)] = 1\n        per_res_contact_matrix[(res2 - 1), (res1 - 1)] = 1\n\n    # correlation of residue to itself is 1.\n    np.fill_diagonal(per_res_contact_matrix, 1)\n\n    if out_file is not None:\n        np.savetxt(out_file, per_res_contact_matrix, delimiter=\" \", fmt=\"%.2f\")\n        print(f\"{out_file} saved to disk.\")\n\n    return per_res_contact_matrix\n</code></pre>"},{"location":"network_analysis/#key_interactions_finder.network_analysis.CorrelationNetwork.gen_res_correl_matrix","title":"<code>gen_res_correl_matrix(out_file=None)</code>","text":"<p>For every residue to every other residue determine the interaction (if exists) with the strongest correlation between them and use it to build a per residue correlation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>out_file</code> <code>Optional[str]</code> <p>Path to save the corelation matrix to. If left empty no file is saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A symmetrical matrix (along diagonal) of correlations between each residue.</p> Source code in <code>key_interactions_finder/network_analysis.py</code> <pre><code>def gen_res_correl_matrix(self, out_file: Optional[str] = None) -&gt; np.ndarray:\n    \"\"\"\n    For every residue to every other residue determine the interaction (if exists)\n    with the strongest correlation between them and use it to build a per residue\n    correlation matrix.\n\n    Parameters\n    ----------\n\n    out_file : Optional[str]\n        Path to save the corelation matrix to. If left empty no file is saved.\n\n    Returns\n    ----------\n\n    np.ndarray\n        A symmetrical matrix (along diagonal) of correlations between each residue.\n    \"\"\"\n    # Generate an empty correlation matrix for all residues.\n    last_residue = self._get_last_residue()\n    per_res_corr_matrix = np.zeros((last_residue, last_residue), dtype=float)\n\n    # Filter correlation matrix to only include columns with specific res number.\n    for res1 in range(1, last_residue + 1):\n        res1_regex_key = self._build_regex_strs(res_number=res1)\n        res1_matrix = self.feature_corr_matrix.filter(regex=res1_regex_key, axis=1)\n\n        # Filter matrix on other axis so matrix contains only the pairs of residues.\n        if len(res1_matrix.columns) != 0:\n            for res2 in range(res1 + 1, last_residue + 1):\n                res2_regex_key = self._build_regex_strs(res_number=res2)\n                res1_res2_matrix = res1_matrix.filter(regex=res2_regex_key, axis=0)\n\n                if len(res1_res2_matrix) != 0:\n                    correls = res1_res2_matrix.to_numpy()\n\n                    try:\n                        # prevent identical interactions (== 1) being used.\n                        correls = correls[correls != 1]\n                        max_correl = max(correls.min(), correls.max(), key=abs)\n\n                        per_res_corr_matrix[(res1 - 1), (res2 - 1)] = max_correl\n                        per_res_corr_matrix[(res2 - 1), (res1 - 1)] = max_correl\n\n                    # ValueError will happen if array becomes empty\n                    # when only identical interactions were present in matrix.\n                    except ValueError:\n                        pass  # value stays at 0.\n\n    # correlation of residue to itself is 1.\n    np.fill_diagonal(per_res_corr_matrix, 1)\n\n    if out_file is not None:\n        np.savetxt(out_file, per_res_corr_matrix, delimiter=\" \", fmt=\"%.2f\")\n        print(f\"{out_file} saved to disk.\")\n\n    return per_res_corr_matrix\n</code></pre>"},{"location":"network_analysis/#key_interactions_finder.network_analysis.heavy_atom_contact_map_from_multiple_pdbs","title":"<code>heavy_atom_contact_map_from_multiple_pdbs(pdb_files, first_res, last_res, d_cut=4.5, out_file=None)</code>","text":"<p>Use MDAnalysis to generate a heavy atom contact map/matrix given a list of PDB files. If 'out_file' specified the result will be saved to disk.</p> <p>Parameters:</p> Name Type Description Default <code>pdb_file</code> <p>Paths to PDB files to generate the contact map from.</p> required <code>first_res</code> <code>int</code> <p>First residue number to use for the contact map.</p> required <code>last_res</code> <code>int</code> <p>Last residue number to use for the contact map.</p> required <code>d_cut</code> <code>Optional[float]</code> <p>Distance cut-off in \u00c5. Default is 4.5 \u00c5.</p> <code>4.5</code> <code>out_file</code> <code>Optional[str]</code> <p>Path to save the contact map file to. If left empty no file is saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Symmetrical (along diagonal) matrix of 1s (in contact) and 0s (not in contact).</p> Source code in <code>key_interactions_finder/network_analysis.py</code> <pre><code>def heavy_atom_contact_map_from_multiple_pdbs(\n    pdb_files: list,\n    first_res: int,\n    last_res: int,\n    d_cut: Optional[float] = 4.5,\n    out_file: Optional[str] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Use MDAnalysis to generate a heavy atom contact map/matrix given a list of PDB files.\n    If 'out_file' specified the result will be saved to disk.\n\n    Parameters\n    ----------\n\n    pdb_file: list\n        Paths to PDB files to generate the contact map from.\n\n    first_res : int\n        First residue number to use for the contact map.\n\n    last_res : int\n        Last residue number to use for the contact map.\n\n    d_cut : Optional[float]\n        Distance cut-off in \u00c5. Default is 4.5 \u00c5.\n\n    out_file : Optional[str]\n        Path to save the contact map file to. If left empty no file is saved.\n\n    Returns\n    ----------\n\n    np.ndarray\n        Symmetrical (along diagonal) matrix of 1s (in contact) and 0s (not in contact).\n    \"\"\"\n    res_selection = \"not name H* and resid \" + str(first_res) + \"-\" + str(last_res)\n\n    all_universes = [mda.Universe(pdb) for pdb in pdb_files]\n\n    all_group1s = [all_universes[idx].select_atoms(res_selection) for idx, _ in enumerate(all_universes)]\n    all_group2s = [all_universes[idx].select_atoms(res_selection) for idx, _ in enumerate(all_universes)]\n\n    matrix_size = (last_res - first_res) + 1\n    per_res_contact_map = np.zeros((matrix_size, matrix_size), dtype=int)\n\n    for group1_idx in range(first_res, last_res + 1):\n        group1_selection = \"resid \" + str(group1_idx)\n        residue_1s = [all_group1s[idx].select_atoms(group1_selection) for idx, _ in enumerate(all_group1s)]\n\n        for group2_idx in range(group1_idx, last_res + 1):\n            group2_selection = \"resid \" + str(group2_idx)\n            residue_2s = [all_group1s[idx].select_atoms(group2_selection) for idx, _ in enumerate(all_group2s)]\n\n            # Find the smallest distance between the residue pairs across all pdbs\n            min_dist_all_pdbs = 999  # always going to be above cut-off.\n            for idx, _ in enumerate(all_group1s):\n                dist_arr = distances.distance_array(\n                    residue_1s[idx].positions,\n                    residue_2s[idx].positions,\n                    box=all_universes[idx].dimensions,\n                )\n                min_dist = dist_arr.min()\n\n                if min_dist &lt; min_dist_all_pdbs:\n                    min_dist_all_pdbs = min_dist\n\n            # Replace matrix pos with 1 if min_dist_all_pdbs less than cutoff.\n            if min_dist_all_pdbs &lt;= d_cut:\n                per_res_contact_map[(group1_idx - 1), (group2_idx - 1)] = 1\n                per_res_contact_map[(group2_idx - 1), (group1_idx - 1)] = 1\n\n    if out_file is not None:\n        np.savetxt(out_file, per_res_contact_map, delimiter=\" \", fmt=\"%.1f\")\n        print(f\"{out_file} saved to disk.\")\n\n    return per_res_contact_map\n</code></pre>"},{"location":"network_analysis/#key_interactions_finder.network_analysis.heavy_atom_contact_map_from_pdb","title":"<code>heavy_atom_contact_map_from_pdb(pdb_file, first_res, last_res, d_cut=4.5, out_file=None)</code>","text":"<p>Use MDAnalysis to generate a heavy atom contact map/matrix given a single PDB file. If 'out_file' specified the result will be saved to disk.</p> <p>Parameters:</p> Name Type Description Default <code>pdb_file</code> <code>str</code> <p>Path to PDB file to generate the contact map from.</p> required <code>first_res</code> <code>int</code> <p>First residue number to use for the contact map.</p> required <code>last_res</code> <code>int</code> <p>Last residue number to use for the contact map.</p> required <code>d_cut</code> <code>Optional[float]</code> <p>Distance cut-off in \u00c5. Default is 4.5 \u00c5.</p> <code>4.5</code> <code>out_file</code> <code>Optional[str]</code> <p>Path to save the contact map file to. If left empty no file is saved.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Symmetrical (along diagonal) matrix of 1s (in contact) and 0s (not in contact).</p> Source code in <code>key_interactions_finder/network_analysis.py</code> <pre><code>def heavy_atom_contact_map_from_pdb(\n    pdb_file: str,\n    first_res: int,\n    last_res: int,\n    d_cut: Optional[float] = 4.5,\n    out_file: Optional[str] = None,\n) -&gt; np.ndarray:\n    \"\"\"\n    Use MDAnalysis to generate a heavy atom contact map/matrix given a single PDB file.\n    If 'out_file' specified the result will be saved to disk.\n\n    Parameters\n    ----------\n\n    pdb_file: str\n        Path to PDB file to generate the contact map from.\n\n    first_res : int\n        First residue number to use for the contact map.\n\n    last_res : int\n        Last residue number to use for the contact map.\n\n    d_cut : Optional[float]\n        Distance cut-off in \u00c5. Default is 4.5 \u00c5.\n\n    out_file : Optional[str]\n        Path to save the contact map file to. If left empty no file is saved.\n\n    Returns\n    ----------\n\n    np.ndarray\n        Symmetrical (along diagonal) matrix of 1s (in contact) and 0s (not in contact).\n    \"\"\"\n    universe = mda.Universe(pdb_file)\n    res_selection = \"not name H* and resid \" + str(first_res) + \"-\" + str(last_res)\n    group1 = universe.select_atoms(res_selection)\n    group2 = universe.select_atoms(res_selection)\n    matrix_size = (last_res - first_res) + 1\n\n    per_res_contact_map = np.zeros((matrix_size, matrix_size), dtype=int)\n\n    for group1_idx in range(first_res, last_res + 1):\n        group1_selection = \"resid \" + str(group1_idx)\n        res1 = group1.select_atoms(group1_selection)\n\n        for group2_idx in range(group1_idx, last_res + 1):\n            group2_selection = \"resid \" + str(group2_idx)\n            res2 = group2.select_atoms(group2_selection)\n\n            # Determine all heavy atom distance between residue pairs.\n            dist_arr = distances.distance_array(res1.positions, res2.positions, box=universe.dimensions)\n\n            # Replace matrix pos with 1 if min_dist less than cutoff.\n            min_dist = dist_arr.min()\n            if min_dist &lt;= d_cut:\n                per_res_contact_map[(group1_idx - 1), (group2_idx - 1)] = 1\n                per_res_contact_map[(group2_idx - 1), (group1_idx - 1)] = 1\n\n    if out_file is not None:\n        np.savetxt(out_file, per_res_contact_map, delimiter=\" \", fmt=\"%.1f\")\n        print(f\"{out_file} saved to disk.\")\n    return per_res_contact_map\n</code></pre>"},{"location":"project_structure_utils/","title":"Project Structure Utils","text":"<p>Note: If you are an end user, you do not need to use this module directly.</p> <p>A set of functions to help with projecting the KIF results onto 3D protein structures.</p> <p>These functions are used to help create the PyMOL and ChimeraX projections of per interaction and per residue scores.</p>"},{"location":"pycontact_processing/","title":"Note","text":"<p>Since KIF V. 0.2 the module pycontact_processing is no longer required. </p> <p>PyContact (which this module was responsible for postprocessing the results of) is an external dependancy and no longer actively maintained. KIF now has the ability to find all contacts in the same manner as PyContact can. This is done through the contact_identification module instead. </p> <p>Documentation for the pycontact_processing module is below.</p>"},{"location":"pycontact_processing/#end-of-note","title":"End of Note","text":"<p>Reformats and (optionally) merges PyContact datafiles generated using either the (1) custom python script provided with this package or (2) the PyContact GUI.</p> <p>Special Notes 1. A pycontact job run with overlapping residue selections can obtain many false interactions, e.g.:     - A vdw interaction with a residue to itself.     - Duplicate interactions with the only difference being residue ordering swapped.</p> <p>If a job is therefore run in the above way, the 'remove_false_interactions' parameter must be set to True (the default) when the class is initialised in order to remove these false interactions. There is no harm if a PyContact job is not run in this way but 'remove_false_interactions' is set to True anyway.</p> <ol> <li>Depending on the md trajectory format used, the residue numbers assigned by PyContact can be off by 1 residue number. If this happens to you, you can use the function \"modify_column_residue_numbers\" to edit/renumber all the features in your dataframe.</li> </ol>"},{"location":"pycontact_processing/#key_interactions_finder.pycontact_processing.PyContactInitializer","title":"<code>PyContactInitializer</code>  <code>dataclass</code>","text":"<p>Handles PyContact output files generated by the custom Python script or the PyContact GUI. (Can merge, reformat and clean duplicates/false interactions).</p> <p>Attributes:</p> Name Type Description <code>pycontact_files</code> <code>str or list</code> <p>String for a single file or list of strings for many files</p> <code>multiple_files</code> <code>bool</code> <p>True or False, do you have multiple files to merge together.</p> <code>merge_files_method</code> <code>Optional[str]</code> <p>If you have multiple files, clarify if they should be merged in a 'vertical' or 'horizontal' manner. If 'multiple_files' = True, must be specfied.</p> <code>remove_false_interactions</code> <code>bool</code> <p>Whether to run a method to clean false interactions from the input datasets. Default is True.</p> <code>in_dir</code> <code>str</code> <p>Directory where the input files are stored. Default is \"\".</p> <code>pycontact_output_type</code> <code>str</code> <p>Define whether the PyContact output files were made using the custom script provided by us (recommended) or using the PyContact GUI. Options are: \"custom_script\" or \"GUI\". Default is \"custom_script\"</p> <code>prepared_df</code> <code>DataFrame</code> <p>Final processed dataframe of all PyContact features. Generated once class is initialized.</p> Source code in <code>key_interactions_finder/pycontact_processing.py</code> <pre><code>@dataclass\nclass PyContactInitializer:\n    \"\"\"\n    Handles PyContact output files generated by the custom Python script or the\n    PyContact GUI.\n    (Can merge, reformat and clean duplicates/false interactions).\n\n    Attributes\n    ----------\n\n    pycontact_files : str or list\n        String for a single file or list of strings for many files\n\n    multiple_files: bool\n        True or False, do you have multiple files to merge together.\n\n    merge_files_method: Optional[str]\n        If you have multiple files, clarify if they should be merged in a\n        'vertical' or 'horizontal' manner.\n        If 'multiple_files' = True, must be specfied.\n\n    remove_false_interactions: bool\n        Whether to run a method to clean false interactions from the input datasets.\n        Default is True.\n\n    in_dir : str\n        Directory where the input files are stored.\n        Default is \"\".\n\n    pycontact_output_type : str\n        Define whether the PyContact output files were made using the custom script provided\n        by us (recommended) or using the PyContact GUI.\n        Options are: \"custom_script\" or \"GUI\".\n        Default is \"custom_script\"\n\n    prepared_df : pd.DataFrame\n        Final processed dataframe of all PyContact features.\n        Generated once class is initialized.\n    \"\"\"\n\n    # Generated when instantiated.\n    pycontact_files: Union[str, list]\n    multiple_files: bool\n    merge_files_method: Optional[str] = None\n    remove_false_interactions: bool = True\n    in_dir: str = \"\"\n    pycontact_output_type: str = \"custom_script\"\n\n    # Generated later.\n    prepared_df: pd.DataFrame = field(init=False)\n\n    # This is called at the end of the dataclass's initialization procedure.\n    def __post_init__(self):\n        \"\"\"Processes the provided PyContact files.\"\"\"\n        if (self.in_dir != \"\") and (self.in_dir[-1] != \"/\"):\n            self.in_dir += \"/\"\n\n        if self.multiple_files:\n            individ_dfs = [self._load_pycontact_dataset(i) for i in self.pycontact_files]\n\n            if self.merge_files_method == \"vertical\":\n                full_df = self._merge_pycontact_datasets_vertically(individ_dfs)\n            elif self.merge_files_method == \"horizontal\":\n                full_df = self._merge_pycontact_datasets_horizontally(individ_dfs)\n            else:\n                error_message = (\n                    \"You said you had multiple files but you did not define the \"\n                    + \"'merge_files_method' parameter as either 'vertical' or 'horizontal'.\"\n                )\n                raise ValueError(error_message)\n        else:\n            full_df = self._load_pycontact_dataset(self.pycontact_files)\n\n        if self.remove_false_interactions:\n            self.prepared_df = self._rm_false_interactions(full_df)\n        else:\n            self.prepared_df = full_df\n\n        num_feats = len(self.prepared_df.columns)\n        num_obs = len(self.prepared_df)\n        print(\"Your PyContact file(s) have been succefully processed.\")\n        print(f\"You have {num_feats} features and {num_obs} observations.\")\n        print(\"The fully processed dataframe is accesible from the '.prepared_df' class attribute.\")\n\n    def _load_pycontact_dataset(self, input_file: str) -&gt; pd.DataFrame:\n        \"\"\"\n        Load a single PyContact dataset into a dataframe.\n\n        Dataset can either be generated by the custom script provided with this package\n        or be generated by the PyContact GUI.\n\n        Parameters\n        ----------\n\n        input_file: str\n            Input file name.\n\n        Returns\n        ----------\n\n        pd.DataFrame\n            A df with each interaction found by PyContact a row in the dataframe.\n        \"\"\"\n        file_in_path = Path(self.in_dir, input_file)\n\n        if self.pycontact_output_type == \"custom_script\":\n            return pd.read_csv(file_in_path)\n\n        if self.pycontact_output_type == \"GUI\":\n            return self._process_gui_file(pycontact_gui_file=file_in_path)\n\n        error_message = (\n            \"You must choose between either 'custom_script' or 'GUI' \" + \"for the parameter pycontact_output_type.\"\n        )\n        raise ValueError(error_message)\n\n    def _process_gui_file(self, pycontact_gui_file: str):\n        \"\"\"\n        Process a PyContact file generated by the PyContact GUI.\n\n        Parameters\n        ----------\n\n        pycontact_gui_file: str\n            complete file path to the GUI generated file.\n\n        Returns\n        ----------\n\n        pd.DataFrame\n            A dataframe with all interactions in the gui file and interactions names\n            reformatted to match the format used throughout this programm.\n        \"\"\"\n        with open(pycontact_gui_file, \"r\", encoding=\"utf-8\") as file:\n            filedata = file.read()\n\n        filedata = filedata.replace(\"[\", \",\").replace(\"]\", \",\")\n        # Removing any double or more spaces and tabs etc...\n        filedata = re.sub(\" +\", \" \", filedata)\n\n        # Standardize formatting.\n        filedata = (\n            filedata.replace(\"hbond\", \"Hbond,\")\n            .replace(\"hydrophobic\", \"Hydrophobic,\")\n            .replace(\"other\", \"Other,\")\n            .replace(\"saltbr\", \"Saltbr,\")\n        )\n        file_data_list = (filedata.split(\"\\n\"))[1:]  # skip top row of headers.\n\n        all_features = {}\n        for line in file_data_list:\n            feature = line.split(\",\")[0]\n\n            # [1], contains the unwanted averages, stdevs etc...\n            scores_str = line.split(\",\")[2:]\n            scores_flt = []\n            for score in scores_str:\n                try:\n                    scores_flt.append(round(float(score), 5))\n                except ValueError:\n                    # Handles ValueError: could not convert string to float: ''\n                    # This occurs if user also requested to output hbond occupancy data.\n                    # These (unwanted) occpancies come after the ValueError,\n                    # so break removes them if they are present in the file.\n                    break\n\n            if scores_flt:  # otherwise get an empty feature at the end.\n                feature_cleaned = self._clean_gui_feature_name(feature_name=feature)\n                all_features.update({feature_cleaned: scores_flt})\n\n        return pd.DataFrame.from_dict(all_features)\n\n    @staticmethod\n    def _merge_pycontact_datasets_horizontally(individ_dfs: list) -&gt; pd.DataFrame:\n        \"\"\"\n        Function to merge multiple PyContact dataframes horizontally.\n\n        This would be used when a user has analysed different parts of their protein\n        with PyContact and wants to put them all together (i.e. each file is from the\n        same trajectory/trajectories.)\n\n        Parameters\n        ----------\n\n        individ_dfs: list\n            List of dataframes to merge.\n\n        Returns\n        ----------\n\n        pd.DataFrame\n            A complete dataframe with the individual dfs merged.\n        \"\"\"\n        df_lengths = [len(df) for df in individ_dfs]\n\n        if len(set(df_lengths)) != 1:\n            except_message = (\n                \"The number of rows in each of your datasets are not identical. \"\n                + \"This is weird because you asked to merge your files horizontally. \"\n                + \"If you are using this approach then your different files should all be from the \"\n                + \"same trajectory just with different contacts measured in each of them. \"\n                + \"If not, you likely want to set the 'merge_files_method' parameter to 'vertical'.\"\n            )\n            raise ValueError(except_message)\n\n        merged_df = pd.concat(individ_dfs, axis=1)\n        merged_df = merged_df.fillna(0.0)\n        return merged_df\n\n    @staticmethod\n    def _merge_pycontact_datasets_vertically(individ_dfs: list) -&gt; pd.DataFrame:\n        \"\"\"\n        Function to merge multiple PyContact dfs vertically. This would be used when a user has\n        multiple replicas or has broken their trajectories into blocks of separate frames.\n\n        Parameters\n        ----------\n\n        individ_dfs: list\n            List of dataframes to merge.\n\n        Returns\n        ----------\n\n        pd.DataFrame\n            Complete df with individual dfs merged in the same order as they were provided.\n        \"\"\"\n        merged_df = pd.concat(individ_dfs, ignore_index=\"True\", sort=\"False\")\n        merged_df = merged_df.fillna(0.0)\n        return merged_df\n\n    @staticmethod\n    def _interaction_is_duplicate(contact_parts: list, contacts_to_keep: list) -&gt; bool:\n        \"\"\"\n        Check if current interaction is a duplicate of an already kept contact.\n\n        Most duplicates are identical but have the residue order swapped.\n        i.e. resX + resY + [other columns] vs. resY + resX + [other columns]\n        This is true if the interaction type is sc-sc or bb-bb.\n\n        Some duplicates don't just have the residue order swapped but as they are of type:\n        sc-bb or bb-sc the test for a duplicate needs to be different.\n        Here, \"contact_parts[5]\" will NOT be identical if they are duplicates because\n        the residue ordering is swapped in the duplicates!\n\n        Parameters\n        ----------\n\n        contact_parts : list\n            The feature/contact to test whether it is a duplicate of a pre-existing contact.\n            The list items are different parts of the contact.\n\n        contacts_to_keep : list[list]\n            A list of lists, out list is all current features that will be kept.\n            Each feature is also a list in same format as contact_parts.\n            (Used to determine if new feature is duplicate of any of these.)\n\n        Returns\n        ----------\n\n        bool\n            True if contact is a duplicate.\n        \"\"\"\n        duplicate = False\n\n        for saved_contact in contacts_to_keep:\n            if contact_parts[5] in (\"sc-sc\", \"bb-bb\"):\n                if (\n                    (contact_parts[2] == saved_contact[0])\n                    and (contact_parts[0] == saved_contact[2])\n                    and (contact_parts[4] == saved_contact[4])\n                    and (contact_parts[5] == saved_contact[5])\n                ):\n                    duplicate = True\n                    break\n\n            else:\n                if (\n                    (contact_parts[2] == saved_contact[0])\n                    and (contact_parts[0] == saved_contact[2])\n                    and (contact_parts[4] == saved_contact[4])\n                    and (contact_parts[5] != saved_contact[5])\n                ):\n                    duplicate = True\n                    break\n\n        return duplicate\n\n    def _rm_false_interactions(self, full_df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Remove non-meaningful (too close to one another) or duplicate contacts/features.\n        Required if in the PyContact job run a user sets the second residue selection\n        group to be something other than \"self\" and the residue selections overlap.\n        Doesn't hurt to be run if not anyway.\n\n        Parameters\n        ----------\n\n        full_df : pd.DataFrame\n            Dataframe of pycontact data (already merged if needed) but not cleaned/proccessed\n            yet.\n\n        Returns\n        ----------\n\n        pd.DataFrame\n            Dataframe with \"false interactions\" removed.\n        \"\"\"\n        contacts_to_del = []\n        contacts_to_keep = []\n        column_names = full_df.columns\n\n        for idx, contact in enumerate(column_names):\n            contact_parts = re.split(r\"(\\d+|\\s)\", contact)\n            # remove the list items with empty or single spaces from the above regex.\n            for list_index in sorted([0, 3, 4, 7, 9], reverse=True):\n                del contact_parts[list_index]\n\n            residue_gap = abs(int(contact_parts[0]) - int(contact_parts[2]))\n            # Remove vdw type interactions if &lt;= 3 residues of each other.\n            if contact_parts[4] in [\"Other\", \"Hydrophobic\"]:\n                if residue_gap &lt;= 3:\n                    contacts_to_del.append(idx)\n                else:\n                    contacts_to_keep.append(contact_parts)\n            # Remove Hbond and Saltbr if &lt;= 2 residues of each other.\n            else:\n                if residue_gap &lt;= 2:\n                    contacts_to_del.append(idx)\n                else:\n                    contacts_to_keep.append(contact_parts)\n\n            if self._interaction_is_duplicate(contact_parts, contacts_to_keep):\n                contacts_to_del.append(idx)\n\n        prepared_df = full_df.drop(full_df.columns[contacts_to_del], axis=1)\n        return prepared_df\n\n    @staticmethod\n    def _clean_gui_feature_name(feature_name: str) -&gt; str:\n        \"\"\"\n        Reformats a feature name from how it is labelled in the GUI output of PyContact\n        to the standardized way that is expected throughout this program.\n\n        Note that with the GUI, it is not possible to save whether the interaction\n        is between the sidechain or backbone for both residues,\n        so for consistency with the rest of program, all features\n        are assinged as \"bb-bb\" (backbone-backbone) interactions.\n\n        Parameters\n        ----------\n\n        feature_name: str\n            GUI formatted feature to reformat.\n\n        Returns\n        ----------\n\n        str\n            Reformatted feature name.\n        \"\"\"\n        res1 = feature_name.split(\"-\")[0]\n        res1_numb = re.findall(r\"\\d+\", res1)[0]\n        res1_name = (re.findall(r\"[A-Z|a-z]+\", res1)[0]).capitalize()\n\n        res2_plus_info = feature_name.split(\"-\")[1]\n        res2_numb = re.findall(r\"\\d+\", res2_plus_info)[0]\n        res2_name = (re.findall(r\"[A-Z|a-z]+\", res2_plus_info)[0]).capitalize()\n\n        # info is interaction type (e.g. Hbond, Vdw etc...)\n        info = (re.findall(r\"[A-Z|a-z]+\", res2_plus_info)[1]).capitalize()\n\n        return res1_numb + res1_name + \" \" + res2_numb + res2_name + \" \" + info + \" bb-bb\"\n</code></pre>"},{"location":"pycontact_processing/#key_interactions_finder.pycontact_processing.PyContactInitializer.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Processes the provided PyContact files.</p> Source code in <code>key_interactions_finder/pycontact_processing.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Processes the provided PyContact files.\"\"\"\n    if (self.in_dir != \"\") and (self.in_dir[-1] != \"/\"):\n        self.in_dir += \"/\"\n\n    if self.multiple_files:\n        individ_dfs = [self._load_pycontact_dataset(i) for i in self.pycontact_files]\n\n        if self.merge_files_method == \"vertical\":\n            full_df = self._merge_pycontact_datasets_vertically(individ_dfs)\n        elif self.merge_files_method == \"horizontal\":\n            full_df = self._merge_pycontact_datasets_horizontally(individ_dfs)\n        else:\n            error_message = (\n                \"You said you had multiple files but you did not define the \"\n                + \"'merge_files_method' parameter as either 'vertical' or 'horizontal'.\"\n            )\n            raise ValueError(error_message)\n    else:\n        full_df = self._load_pycontact_dataset(self.pycontact_files)\n\n    if self.remove_false_interactions:\n        self.prepared_df = self._rm_false_interactions(full_df)\n    else:\n        self.prepared_df = full_df\n\n    num_feats = len(self.prepared_df.columns)\n    num_obs = len(self.prepared_df)\n    print(\"Your PyContact file(s) have been succefully processed.\")\n    print(f\"You have {num_feats} features and {num_obs} observations.\")\n    print(\"The fully processed dataframe is accesible from the '.prepared_df' class attribute.\")\n</code></pre>"},{"location":"pycontact_processing/#key_interactions_finder.pycontact_processing.modify_column_residue_numbers","title":"<code>modify_column_residue_numbers(dataset, constant_to_add=1)</code>","text":"<p>Take a dataframe of PyContact generated features and add a constant value to all residue numbers in each feature. This function exists as in some cases mdanalysis (used by PyContact) may renumber residue numbers to start from 0 as opposed to most MD engines which start from 1.</p> <p>This function will NOT update the class attribute \".prepared_df\". Instead it returns a new dataframe with the updated residue numbers.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>DataFrame</code> <p>Input dataframe with Pycontact features you wish to modify.</p> required <code>constant_to_add</code> <code>int</code> <p>Value of the constant you want to add from each residue. You can use negative numbers to subtract. Default = 1</p> <code>1</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with residue numbers updated accordingly.</p> Source code in <code>key_interactions_finder/pycontact_processing.py</code> <pre><code>def modify_column_residue_numbers(dataset: pd.DataFrame, constant_to_add: int = 1) -&gt; pd.DataFrame:\n    \"\"\"\n    Take a dataframe of PyContact generated features and add a constant value to all residue\n    numbers in each feature. This function exists as in some cases mdanalysis (used by PyContact)\n    may renumber residue numbers to start from 0 as opposed to most MD engines which\n    start from 1.\n\n    This function will NOT update the class attribute \".prepared_df\".\n    Instead it returns a new dataframe with the updated residue numbers.\n\n    Parameters\n    ----------\n\n    dataset: pd.DataFrame\n        Input dataframe with Pycontact features you wish to modify.\n\n    constant_to_add: int\n        Value of the constant you want to add from each residue.\n        You can use negative numbers to subtract.\n        Default = 1\n\n    Returns\n    ----------\n\n    pd.DataFrame\n        Dataframe with residue numbers updated accordingly.\n    \"\"\"\n    updated_names = []\n    all_ori_names = list(dataset)\n    for column_name in all_ori_names:\n        res_split = re.split(r\"(\\d+)\", column_name)\n\n        res1_numb = int(res_split[1])\n        res1_name = res_split[2]\n        res2_numb = int(res_split[3])\n        remainder = res_split[4]\n\n        res1_numb += constant_to_add\n        res2_numb += constant_to_add\n\n        updated_name = str(res1_numb) + res1_name + str(res2_numb) + remainder\n        updated_names.append(updated_name)\n\n    # don't want to overwrite class dataframe in case user runs twice or by accident etc...\n    new_dataset = dataset.copy(deep=True)\n    new_dataset.columns = updated_names\n    return new_dataset\n</code></pre>"},{"location":"pymol_projections/","title":"Pymol Projections","text":"<p>Creates PyMOL compatible python scripts to visualise user generated results on a 3D model of the protein.</p> <p>4 Functions available for the end user:</p> <ol> <li> <p>project_pymol_per_res_scores(per_res_scores, model_name, out_dir)     Write out a PyMOL compatible python script to project the per residue scores.</p> </li> <li> <p>project_multiple_per_res_scores(all_per_res_scores, out_dir)     Write out multiple PyMOL compatible visualisation scripts for     the per residue scores, one script for each model used.</p> </li> <li> <p>project_pymol_top_features()     Write out a PyMOL compatible python script to project the top features.</p> </li> <li> <p>project_multiple_per_feature_scores(all_feature_scores, numb_features, out_dir)     Write out multiple PyMOL compatible scripts for different models.</p> </li> </ol>"},{"location":"pymol_projections/#key_interactions_finder.pymol_projections.project_multiple_per_feature_scores","title":"<code>project_multiple_per_feature_scores(all_per_feature_scores, numb_features, out_dir='')</code>","text":"<p>Write out multiple PyMOL compatible scripts for different models.</p> <p>Parameters:</p> Name Type Description Default <code>all_per_feature_scores</code> <code>dict</code> <p>Nested dictionary, the outer layer keys are the model names/methods used. The inner layer is a dict with keys being each residue and values the per residue score.</p> required <code>numb_features</code> <code>int or str</code> <p>The max number of top scoring features to determine (specified by an int). Alternatively, if set to \"all\", then all per feature scores will be determined.</p> required <code>out_dir</code> <code>str</code> <p>Folder to save outputs to, if none given, saved to current directory.</p> <code>''</code> Source code in <code>key_interactions_finder/pymol_projections.py</code> <pre><code>def project_multiple_per_feature_scores(\n    all_per_feature_scores: dict, numb_features: Union[int, str], out_dir: str = \"\"\n) -&gt; None:\n    \"\"\"\n    Write out multiple PyMOL compatible scripts for different models.\n\n    Parameters\n    ----------\n\n    all_per_feature_scores : dict\n        Nested dictionary, the outer layer keys are the model names/methods used.\n        The inner layer is a dict with keys being each residue and\n        values the per residue score.\n\n    numb_features : int or str\n        The max number of top scoring features to determine (specified by an int).\n        Alternatively, if set to \"all\", then all per feature scores will be determined.\n\n    out_dir : str\n        Folder to save outputs to, if none given, saved to current directory.\n    \"\"\"\n    for model_name, model_scores in all_per_feature_scores.items():\n        project_pymol_top_features(\n            per_feature_scores=model_scores, model_name=model_name, numb_features=numb_features, out_dir=out_dir\n        )\n</code></pre>"},{"location":"pymol_projections/#key_interactions_finder.pymol_projections.project_multiple_per_res_scores","title":"<code>project_multiple_per_res_scores(all_per_res_scores, out_dir='')</code>","text":"<p>Write out multiple PyMOL compatible visualisation scripts for the per residue scores, one script for each model used.</p> <p>Parameters:</p> Name Type Description Default <code>all_per_res_scores</code> <code>dict</code> <p>Nested dictionary, the outer layer keys are the model names/methods used. The inner layer is a dict with keys being each residue and values the per residue score.</p> required <code>out_dir</code> <code>str</code> <p>Folder to save outputs to, if none given, saved to current directory.</p> <code>''</code> Source code in <code>key_interactions_finder/pymol_projections.py</code> <pre><code>def project_multiple_per_res_scores(all_per_res_scores: dict, out_dir: str = \"\") -&gt; None:\n    \"\"\"\n    Write out multiple PyMOL compatible visualisation scripts for\n    the per residue scores, one script for each model used.\n\n    Parameters\n    ----------\n\n    all_per_res_scores : dict\n        Nested dictionary, the outer layer keys are the model names/methods used.\n        The inner layer is a dict with keys being each residue and\n        values the per residue score.\n\n    out_dir : str\n        Folder to save outputs to, if none given, saved to current directory.\n    \"\"\"\n    for model_name, model_scores in all_per_res_scores.items():\n        project_pymol_per_res_scores(per_res_scores=model_scores, model_name=str(model_name), out_dir=out_dir)\n</code></pre>"},{"location":"pymol_projections/#key_interactions_finder.pymol_projections.project_pymol_per_res_scores","title":"<code>project_pymol_per_res_scores(per_res_scores, model_name='', out_dir='')</code>","text":"<p>Write out a PyMOL compatible python script to project the per residue scores.</p> <p>Parameters:</p> Name Type Description Default <code>per_res_scores</code> <code>dict</code> <p>The keys are each residue and values the per residue score.</p> required <code>model_name</code> <code>str</code> <p>Appended to start of output file to identify it.</p> <code>''</code> <code>out_dir</code> <code>str</code> <p>Folder to save outputs to, if none given, saved to current directory.</p> <code>''</code> Source code in <code>key_interactions_finder/pymol_projections.py</code> <pre><code>def project_pymol_per_res_scores(per_res_scores: dict, model_name: str = \"\", out_dir: str = \"\") -&gt; None:\n    \"\"\"\n    Write out a PyMOL compatible python script to project the per residue scores.\n\n    Parameters\n    ----------\n\n    per_res_scores : dict\n        The keys are each residue and values the per residue score.\n\n    model_name : str\n        Appended to start of output file to identify it.\n\n    out_dir : str\n        Folder to save outputs to, if none given, saved to current directory.\n    \"\"\"\n    out_dir = _prep_out_dir(out_dir)\n\n    # Header\n    per_res_import_out = \"\"\n    per_res_import_out += \"# You can run me in several ways, perhaps the easiest way is to:\\n\"\n    per_res_import_out += \"# 1. Load the PDB file of your system in PyMOL.\\n\"\n    per_res_import_out += \"# 2. Type: @[FILE_NAME.py] in the command line.\\n\"\n    per_res_import_out += \"# 3. Make sure the .py file is in the same directory as the pdb.\\n\"\n    per_res_import_out += \"set sphere_color, red\\n\"\n    per_res_import_out += \"# The lines below are suggestions for potentially nicer figures.\\n\"\n    per_res_import_out += \"# You can comment them in if you want.\\n\"\n    per_res_import_out += \"# bg_color white\\n\"\n    per_res_import_out += \"# set cartoon_color, grey90\\n\"\n    per_res_import_out += \"# set ray_opaque_background, 0\\n\"\n    per_res_import_out += \"# set antialias, 2\\n\"\n    per_res_import_out += \"# set ray_shadows, 0\\n\"\n\n    # Main, tells PyMOL to show spheres and set their size accordingly.\n    for res_numb, sphere_size in per_res_scores.items():\n        per_res_import_out += f\"show spheres, resi {res_numb} and name CA\\n\"\n        per_res_import_out += f\"set sphere_scale, {sphere_size:.4f}, resi {res_numb} and name CA\\n\"\n\n    # user selection of all CA carbons so easy to modify the sphere colours etc...\n    all_spheres_list = list(per_res_scores.keys())\n    all_spheres_str = \"+\".join(map(str, all_spheres_list))\n    per_res_import_out += f\"sele All_Spheres, resi {all_spheres_str} and name CA\\n\"\n\n    out_file_name = model_name + \"_Pymol_Per_Res_Scores.py\"\n    out_file_path = Path(out_dir, out_file_name)\n    _write_file(out_file_path, per_res_import_out)\n    print(f\"The file: {out_file_path} was written to disk.\")\n</code></pre>"},{"location":"pymol_projections/#key_interactions_finder.pymol_projections.project_pymol_top_features","title":"<code>project_pymol_top_features(per_feature_scores, model_name, numb_features='all', out_dir='')</code>","text":"<p>Write out a PyMOL compatible python script to project the top X features. Features will be shown as cylinders between each residue pair, with cylinder size controlled according to relative score and cylinder colour controlled by interaction type.</p> <p>Parameters:</p> Name Type Description Default <code>per_feature_scores</code> <code>dict</code> <p>Keys are the names of the features and values are their scores.</p> required <code>model_name</code> <code>str</code> <p>What name to appended to the start of the output file name to help identify it.</p> required <code>numb_features</code> <code>int or str</code> <p>The max number of top scoring features to determine (specified by an int). Alternatively, if set to \"all\", then all feature scores will be determined.</p> <code>'all'</code> <code>out_dir</code> <code>str</code> <p>Folder to save outputs to, if none given, saved to current directory.</p> <code>''</code> Source code in <code>key_interactions_finder/pymol_projections.py</code> <pre><code>def project_pymol_top_features(\n    per_feature_scores: dict, model_name: str, numb_features: Union[int, str] = \"all\", out_dir: str = \"\"\n) -&gt; None:\n    \"\"\"\n    Write out a PyMOL compatible python script to project the top X features.\n    Features will be shown as cylinders between each residue pair,\n    with cylinder size controlled according to relative score and\n    cylinder colour controlled by interaction type.\n\n    Parameters\n    ----------\n\n    per_feature_scores : dict\n        Keys are the names of the features and values are their scores.\n\n    model_name : str\n        What name to appended to the start of the output file name to help identify it.\n\n    numb_features : int or str\n        The max number of top scoring features to determine (specified by an int).\n        Alternatively, if set to \"all\", then all feature scores will be determined.\n\n    out_dir : str\n        Folder to save outputs to, if none given, saved to current directory.\n    \"\"\"\n    out_dir = _prep_out_dir(out_dir)\n\n    df_feat_import = df_feat_import = pd.DataFrame(per_feature_scores.items())\n    df_feat_import_res = df_feat_import[0].str.split(\" \", expand=True)\n\n    res1, res2 = _extract_residue_lists(df_feat_import_res)\n    interact_color = _extract_interaction_types(df_feat_import_res)\n    interact_strengths = _scale_interaction_strengths(df_feat_import)\n\n    # Header of output file.\n    top_feats_out = \"\"\n    top_feats_out += \"# You can run me in several ways, perhaps the easiest way is to:\\n\"\n    top_feats_out += \"# 1. Load the PDB file of your system in PyMOL.\\n\"\n    top_feats_out += \"# 2. Type: @[FILE_NAME.py] in the command line.\\n\"\n    top_feats_out += \"# Make sure the .py files are in the same directory as the pdb.\\n\"\n    top_feats_out += \"# The lines below are suggestions for potentially nicer figures.\\n\"\n    top_feats_out += \"# You can comment them in if you want.\\n\"\n    top_feats_out += \"# bg_color white\\n\"\n    top_feats_out += \"# set cartoon_color, grey90\\n\"\n    top_feats_out += \"# set ray_opaque_background, 0\\n\"\n    top_feats_out += \"# set antialias, 2\\n\"\n    top_feats_out += \"# set ray_shadows, 0\\n\"\n\n    # Main, show CA carbons as spheres and set their size.\n    if numb_features == \"all\":\n        numb_features = len(res1)\n    elif not isinstance(numb_features, int):\n        raise ValueError(\"You defined the parameter 'numb_features' as neither 'all' or as an integer.\")\n\n    # prevent issue if user requests more features than exist.\n    numb_features = min(numb_features, len(res1))\n\n    for i in range(numb_features):\n        feature_rep = (\n            f\"distance interaction{i}, \"\n            + f\"resid {str(res1[i])} and name CA, \"\n            + f\"resid {str(res2[i])} and name CA \\n\"\n            + f\"set dash_radius, {interact_strengths[i]}, interaction{i} \\n\"\n            f\"set dash_color, {interact_color[i]}, interaction{i} \\n\"\n        )\n        top_feats_out += feature_rep\n\n    # Finally, group all cylinders made together - easier for user to handle in PyMOL\n    top_feats_out += \"group All_Interactions, interaction* \\n\"\n\n    # general cylinder settings\n    top_feats_out += \"set dash_gap, 0.00, All_Interactions \\n\"\n    top_feats_out += \"set dash_round_ends, off, All_Interactions \\n\"\n    top_feats_out += \"hide labels \\n\"\n\n    out_file_name = model_name + \"_Pymol_Per_Feature_Scores.py\"\n    out_file_path = Path(out_dir, out_file_name)\n    _write_file(out_file_path, top_feats_out)\n    print(f\"The file: {out_file_path} was written to disk.\")\n</code></pre>"},{"location":"stat_modelling/","title":"Statistical Modelling","text":"<p>Calculates differences in the probability distributions of each feature for the different classes. This is only available to supervised datasets (i.e. data must has class labels).</p> <p>2 Classes for end user usage:</p> <ol> <li> <p>ClassificationStatModel     For analysis when the target data is categorical (classification).     Can only use two classes at a time (i.e., binary classification)</p> </li> <li> <p>RegressionStatModel     For analysis when the target data is continuous (regression).</p> </li> </ol> <p>These classes both inherit from a parent class called \"_ProteinStatModel\" which abstracts as much as their shared behavior as possible.</p>"},{"location":"stat_modelling/#key_interactions_finder.stat_modelling.ClassificationStatModel","title":"<code>ClassificationStatModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_ProteinStatModel</code></p> <p>Handles the generation of statistical models for PyContact data sets when the target is made up of two unqiue class labels.</p> <p>Note that most attributes are inherited from _ProteinStatModel.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>DataFrame</code> <p>Input dataframe.</p> <code>class_names</code> <code>list</code> <p>Class labels inside the column \"Target\" of the dataset to model. You can only use two classes for this approach.</p> <code>out_dir</code> <code>str</code> <p>Directory path to store results files to. Default = \"\"</p> <code>interaction_types_included</code> <code>(list, optional)</code> <p>What types of molecular interactions to generate the correlation matrix for. options are one or more of: [\"Hbond\", \"Hydrophobic\", \"Saltbr\", \"Other\"] Default is to include all 4 types.</p> <code>scaled_dataset</code> <code>DataFrame</code> <p>Input dataset with all features scaled.</p> <code>feature_list</code> <code>list</code> <p>List of all feature labels in the dataset.</p> <code>x_values</code> <code>ndarray</code> <p>Values on the x-axis for plotting the kernel density estimations.</p> <code>kdes</code> <code>dict</code> <p>Nested dictionary. Outer layer keys are class names, and values are a dictionary of each feature (as inner key) and values of a nested array of kernel density estimations (kdes).</p> <code>js_distances</code> <code>dict</code> <p>Dictionary with each feature's (keys) and Jensen Shannon distance (values). Dictionary is sorted from largest Jensen Shannon distance to smallest.</p> <code>mutual_infos</code> <code>dict</code> <p>Dictionary with each feature's (keys) and mutual informations (values). Dictionary is sorted from largest mutual information to smallest.</p> <p>Methods:</p> Name Description <code>calc_mutual_info_to_target</code> <p>Calculate the mutual information between each feature to the target classes.</p> <code>calc_js_distances</code> <p>Calculate the Jensen-Shannon (JS) distance (metric) between each feature to the target classes.</p> Source code in <code>key_interactions_finder/stat_modelling.py</code> <pre><code>@dataclass\nclass ClassificationStatModel(_ProteinStatModel):\n    \"\"\"\n    Handles the generation of statistical models for PyContact data sets\n    when the target is made up of two unqiue class labels.\n\n    Note that most attributes are inherited from _ProteinStatModel.\n\n    Attributes\n    ----------\n\n    dataset : pd.DataFrame\n        Input dataframe.\n\n    class_names : list\n        Class labels inside the column \"Target\" of the dataset to model.\n        You can only use two classes for this approach.\n\n    out_dir : str\n        Directory path to store results files to.\n        Default = \"\"\n\n    interaction_types_included : list, optional\n        What types of molecular interactions to generate the correlation matrix for.\n        options are one or more of: [\"Hbond\", \"Hydrophobic\", \"Saltbr\", \"Other\"]\n        Default is to include all 4 types.\n\n    scaled_dataset : pd.DataFrame\n        Input dataset with all features scaled.\n\n    feature_list : list\n        List of all feature labels in the dataset.\n\n    x_values : np.ndarray\n        Values on the x-axis for plotting the kernel density estimations.\n\n    kdes : dict\n        Nested dictionary. Outer layer keys are class names, and values\n        are a dictionary of each feature (as inner key) and values of a\n        nested array of kernel density estimations (kdes).\n\n    js_distances : dict\n        Dictionary with each feature's (keys) and Jensen Shannon distance (values).\n        Dictionary is sorted from largest Jensen Shannon distance to smallest.\n\n    mutual_infos : dict\n        Dictionary with each feature's (keys) and mutual informations (values).\n        Dictionary is sorted from largest mutual information to smallest.\n\n    Methods\n    -------\n\n    calc_mutual_info_to_target(save_result=True)\n        Calculate the mutual information between each feature to the target classes.\n\n    calc_js_distances(kde_bandwidth=0.02, save_result=True)\n        Calculate the Jensen-Shannon (JS) distance (metric) between each feature to\n        the target classes.\n    \"\"\"\n\n    # Generated at initialization.\n    class_names: list = field(default_factory=[])\n    # Generated later.\n    x_values: np.ndarray = field(init=False)\n    kdes: dict = field(init=False)\n    js_distances: dict = field(init=False)\n\n    # Called at the end of the dataclass's initialization procedure.\n    def __post_init__(self) -&gt; None:\n        \"\"\"Filter, rescale and calc the kdes for each feature.\"\"\"\n        self.out_dir = _prep_out_dir(self.out_dir)\n\n        self.x_values = np.empty(shape=(0, 0))\n        self.kdes = {}\n        self.js_distances = {}\n        self.mutual_infos = {}\n\n        if sorted(self.interaction_types_included) != sorted([\"Hbond\", \"Hydrophobic\", \"Saltbr\", \"Other\"]):\n            self.dataset = _filter_features_by_strings(\n                dataset=self.dataset, strings_to_preserve=self.interaction_types_included\n            )\n\n        self.feature_list = list(self.dataset.columns)\n        self.feature_list.remove(\"Target\")\n\n        # Features need to be scaled in order to use same bandwidth throughout.\n        self.scaled_dataset = self._scale_features()\n\n        if len(self.class_names) != 2:\n            raise ValueError(\n                \"The number of classes to compare should be 2. \\n\"\n                + \"Please use a list of 2 items for the parameter: 'class_names'.\"\n            )\n\n    def calc_mutual_info_to_target(self, save_result: bool = True):\n        \"\"\"\n        Calculate the mutual information between each feature to the 2 target classes.\n        Note that Sklearns implementation (used here) is designed for \"raw datasets\"\n        (i.e., do not feed in a probability distribution, instead feed in the observations).\n\n        Further, the mutual information values calculated from Sklearns implementation are\n        scaled by the natural logarithm of 2. In this implementation,\n        the results are re-scaled to be linear.\n\n        Parameters\n        ----------\n\n        save_result : Optional[bool] = True\n            Save result to disk or not.\n            Optional, default is to save.\n        \"\"\"\n        df_features = self.scaled_dataset.drop(\"Target\", axis=1)\n        features_array = df_features.to_numpy()\n        classes = self.scaled_dataset[\"Target\"].to_numpy()\n\n        mutual_info_raw = mutual_info_classif(features_array, classes)\n        mutual_info_rescaled = np.around((np.exp(mutual_info_raw) - 1), 5)\n\n        self.mutual_infos = dict(zip(df_features.columns, mutual_info_rescaled, strict=True))\n        self.mutual_infos = {k: v for k, v in sorted(self.mutual_infos.items(), key=lambda item: item[1], reverse=True)}\n\n        print(\"Mutual information scores calculated.\")\n\n        if save_result:\n            out_file_path = Path(self.out_dir, \"Mutual_Information_Per_Feature_Scores.csv\")\n            self._per_feature_scores_to_file(per_feat_values=self.mutual_infos, out_file=out_file_path)\n            print(\"You can also access these results via the class attribute: 'mutual_infos'.\")\n\n    def calc_js_distances(self, kde_bandwidth: float = 0.02, save_result: bool = True):\n        \"\"\"\n        Calculate the Jensen-Shannon (JS) distance (metric) between each feature to\n        the target classes.\n        Requires that each feature is described by a probabilty distribution.\n\n        Parameters\n        ----------\n\n        kde_bandwidth : Optional[float]\n            Bandwidth used to generate the probabilty distribtions for each feature set.\n            Note that features are all scaled to be between 0 and 1 before this step.\n            Optional, default = 0.02\n\n        save_result : Optional[bool] = True\n            Save result to disk or not.\n            Optional, default is to save.\n        \"\"\"\n        for class_name in self.class_names:\n            # split observations into each class first.\n            per_class_dataset = self.scaled_dataset[(self.scaled_dataset[\"Target\"] == class_name)]\n\n            # generate kdes for each class.\n            self.x_values, self.kdes[class_name] = self._gen_kdes(\n                input_features=per_class_dataset, kde_bandwidth=kde_bandwidth\n            )\n\n        # iterate through each feature and calc js dist.\n        for feature in self.feature_list:\n            distrib_1 = self.kdes[self.class_names[0]][feature]\n            distrib_2 = self.kdes[self.class_names[1]][feature]\n\n            js_dist = np.around(jensenshannon(distrib_1, distrib_2, base=2), 5)\n\n            self.js_distances.update({feature: js_dist})\n\n        self.js_distances = {k: v for k, v in sorted(self.js_distances.items(), key=lambda item: item[1], reverse=True)}\n\n        print(\"Jensen-Shannon (JS) distances calculated.\")\n\n        if save_result:\n            out_file_path = Path(self.out_dir, \"Jensen_Shannon_Per_Feature_Scores.csv\")\n            self._per_feature_scores_to_file(per_feat_values=self.js_distances, out_file=out_file_path)\n\n            print(\"You can also access these results via the class attribute: 'js_distances'.\")\n</code></pre>"},{"location":"stat_modelling/#key_interactions_finder.stat_modelling.ClassificationStatModel.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Filter, rescale and calc the kdes for each feature.</p> Source code in <code>key_interactions_finder/stat_modelling.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Filter, rescale and calc the kdes for each feature.\"\"\"\n    self.out_dir = _prep_out_dir(self.out_dir)\n\n    self.x_values = np.empty(shape=(0, 0))\n    self.kdes = {}\n    self.js_distances = {}\n    self.mutual_infos = {}\n\n    if sorted(self.interaction_types_included) != sorted([\"Hbond\", \"Hydrophobic\", \"Saltbr\", \"Other\"]):\n        self.dataset = _filter_features_by_strings(\n            dataset=self.dataset, strings_to_preserve=self.interaction_types_included\n        )\n\n    self.feature_list = list(self.dataset.columns)\n    self.feature_list.remove(\"Target\")\n\n    # Features need to be scaled in order to use same bandwidth throughout.\n    self.scaled_dataset = self._scale_features()\n\n    if len(self.class_names) != 2:\n        raise ValueError(\n            \"The number of classes to compare should be 2. \\n\"\n            + \"Please use a list of 2 items for the parameter: 'class_names'.\"\n        )\n</code></pre>"},{"location":"stat_modelling/#key_interactions_finder.stat_modelling.ClassificationStatModel.calc_js_distances","title":"<code>calc_js_distances(kde_bandwidth=0.02, save_result=True)</code>","text":"<p>Calculate the Jensen-Shannon (JS) distance (metric) between each feature to the target classes. Requires that each feature is described by a probabilty distribution.</p> <p>Parameters:</p> Name Type Description Default <code>kde_bandwidth</code> <code>Optional[float]</code> <p>Bandwidth used to generate the probabilty distribtions for each feature set. Note that features are all scaled to be between 0 and 1 before this step. Optional, default = 0.02</p> <code>0.02</code> <code>save_result</code> <code>Optional[bool] = True</code> <p>Save result to disk or not. Optional, default is to save.</p> <code>True</code> Source code in <code>key_interactions_finder/stat_modelling.py</code> <pre><code>def calc_js_distances(self, kde_bandwidth: float = 0.02, save_result: bool = True):\n    \"\"\"\n    Calculate the Jensen-Shannon (JS) distance (metric) between each feature to\n    the target classes.\n    Requires that each feature is described by a probabilty distribution.\n\n    Parameters\n    ----------\n\n    kde_bandwidth : Optional[float]\n        Bandwidth used to generate the probabilty distribtions for each feature set.\n        Note that features are all scaled to be between 0 and 1 before this step.\n        Optional, default = 0.02\n\n    save_result : Optional[bool] = True\n        Save result to disk or not.\n        Optional, default is to save.\n    \"\"\"\n    for class_name in self.class_names:\n        # split observations into each class first.\n        per_class_dataset = self.scaled_dataset[(self.scaled_dataset[\"Target\"] == class_name)]\n\n        # generate kdes for each class.\n        self.x_values, self.kdes[class_name] = self._gen_kdes(\n            input_features=per_class_dataset, kde_bandwidth=kde_bandwidth\n        )\n\n    # iterate through each feature and calc js dist.\n    for feature in self.feature_list:\n        distrib_1 = self.kdes[self.class_names[0]][feature]\n        distrib_2 = self.kdes[self.class_names[1]][feature]\n\n        js_dist = np.around(jensenshannon(distrib_1, distrib_2, base=2), 5)\n\n        self.js_distances.update({feature: js_dist})\n\n    self.js_distances = {k: v for k, v in sorted(self.js_distances.items(), key=lambda item: item[1], reverse=True)}\n\n    print(\"Jensen-Shannon (JS) distances calculated.\")\n\n    if save_result:\n        out_file_path = Path(self.out_dir, \"Jensen_Shannon_Per_Feature_Scores.csv\")\n        self._per_feature_scores_to_file(per_feat_values=self.js_distances, out_file=out_file_path)\n\n        print(\"You can also access these results via the class attribute: 'js_distances'.\")\n</code></pre>"},{"location":"stat_modelling/#key_interactions_finder.stat_modelling.ClassificationStatModel.calc_mutual_info_to_target","title":"<code>calc_mutual_info_to_target(save_result=True)</code>","text":"<p>Calculate the mutual information between each feature to the 2 target classes. Note that Sklearns implementation (used here) is designed for \"raw datasets\" (i.e., do not feed in a probability distribution, instead feed in the observations).</p> <p>Further, the mutual information values calculated from Sklearns implementation are scaled by the natural logarithm of 2. In this implementation, the results are re-scaled to be linear.</p> <p>Parameters:</p> Name Type Description Default <code>save_result</code> <code>Optional[bool] = True</code> <p>Save result to disk or not. Optional, default is to save.</p> <code>True</code> Source code in <code>key_interactions_finder/stat_modelling.py</code> <pre><code>def calc_mutual_info_to_target(self, save_result: bool = True):\n    \"\"\"\n    Calculate the mutual information between each feature to the 2 target classes.\n    Note that Sklearns implementation (used here) is designed for \"raw datasets\"\n    (i.e., do not feed in a probability distribution, instead feed in the observations).\n\n    Further, the mutual information values calculated from Sklearns implementation are\n    scaled by the natural logarithm of 2. In this implementation,\n    the results are re-scaled to be linear.\n\n    Parameters\n    ----------\n\n    save_result : Optional[bool] = True\n        Save result to disk or not.\n        Optional, default is to save.\n    \"\"\"\n    df_features = self.scaled_dataset.drop(\"Target\", axis=1)\n    features_array = df_features.to_numpy()\n    classes = self.scaled_dataset[\"Target\"].to_numpy()\n\n    mutual_info_raw = mutual_info_classif(features_array, classes)\n    mutual_info_rescaled = np.around((np.exp(mutual_info_raw) - 1), 5)\n\n    self.mutual_infos = dict(zip(df_features.columns, mutual_info_rescaled, strict=True))\n    self.mutual_infos = {k: v for k, v in sorted(self.mutual_infos.items(), key=lambda item: item[1], reverse=True)}\n\n    print(\"Mutual information scores calculated.\")\n\n    if save_result:\n        out_file_path = Path(self.out_dir, \"Mutual_Information_Per_Feature_Scores.csv\")\n        self._per_feature_scores_to_file(per_feat_values=self.mutual_infos, out_file=out_file_path)\n        print(\"You can also access these results via the class attribute: 'mutual_infos'.\")\n</code></pre>"},{"location":"stat_modelling/#key_interactions_finder.stat_modelling.RegressionStatModel","title":"<code>RegressionStatModel</code>  <code>dataclass</code>","text":"<p>               Bases: <code>_ProteinStatModel</code></p> <p>Handles the generation of statistical models for PyContact data sets when the target variable is continous.</p> <p>Note that several attributes listed below are inherited from _ProteinStatModel.</p> <p>Attributes:</p> Name Type Description <code>dataset</code> <code>DataFrame</code> <p>Input dataframe.</p> <code>out_dir</code> <code>str</code> <p>Directory path to store results files to. Default = \"\"</p> <code>interaction_types_included</code> <code>(list, optional)</code> <p>What types of molecular interactions to generate the correlation matrix for. options are one or more of: [\"Hbond\", \"Hydrophobic\", \"Saltbr\", \"Other\"] Default is to include all 4 types.</p> <code>scaled_dataset</code> <code>DataFrame</code> <p>Input dataset with all features scaled.</p> <code>feature_list</code> <code>list</code> <p>List of all feature labels in the dataset.</p> <code>mutual_infos</code> <code>dict</code> <p>Dictionary with each feature's (keys) and mutual informations (values). Dictionary is sorted from largest mutual information to smallest.</p> <code>linear_correlations</code> <code>dict</code> <p>Dictionary with each feature's (keys) and linear correlations (values). Dictionary is sorted from largest (absolute) linear correlation to smallest.</p> <p>Methods:</p> Name Description <code>calc_mutual_info_to_target</code> <p>Calculate the mutual information between each feature and the target.</p> <code>calc_linear_correl_to_target</code> <p>Calculate the pearson correlation coeffcient between each feature and the target.</p> Source code in <code>key_interactions_finder/stat_modelling.py</code> <pre><code>@dataclass\nclass RegressionStatModel(_ProteinStatModel):\n    \"\"\"\n    Handles the generation of statistical models for PyContact data sets\n    when the target variable is continous.\n\n    Note that several attributes listed below are inherited from _ProteinStatModel.\n\n    Attributes\n    ----------\n\n    dataset : pd.DataFrame\n        Input dataframe.\n\n    out_dir : str\n        Directory path to store results files to.\n        Default = \"\"\n\n    interaction_types_included : list, optional\n        What types of molecular interactions to generate the correlation matrix for.\n        options are one or more of: [\"Hbond\", \"Hydrophobic\", \"Saltbr\", \"Other\"]\n        Default is to include all 4 types.\n\n    scaled_dataset : pd.DataFrame\n        Input dataset with all features scaled.\n\n    feature_list : list\n        List of all feature labels in the dataset.\n\n    mutual_infos : dict\n        Dictionary with each feature's (keys) and mutual informations (values).\n        Dictionary is sorted from largest mutual information to smallest.\n\n    linear_correlations : dict\n        Dictionary with each feature's (keys) and linear correlations (values).\n        Dictionary is sorted from largest (absolute) linear correlation to smallest.\n\n    Methods\n    -------\n\n    calc_mutual_info_to_target(save_result=True)\n        Calculate the mutual information between each feature and the target.\n\n    calc_linear_correl_to_target(save_result=True)\n        Calculate the pearson correlation coeffcient between each feature and the target.\n    \"\"\"\n\n    # Attribute is generated after initiziliation.\n    linear_correlations: dict = field(init=False)\n\n    # Called at the end of the dataclass's initialization procedure.\n    def __post_init__(self) -&gt; None:\n        \"\"\"Filter and rescale the features.\"\"\"\n        self.out_dir = _prep_out_dir(self.out_dir)\n\n        self.mutual_infos = {}\n        self.linear_correlations = {}\n\n        if sorted(self.interaction_types_included) != sorted([\"Hbond\", \"Hydrophobic\", \"Saltbr\", \"Other\"]):\n            self.dataset = _filter_features_by_strings(\n                dataset=self.dataset, strings_to_preserve=self.interaction_types_included\n            )\n\n        self.feature_list = list(self.dataset.columns)\n        self.feature_list.remove(\"Target\")\n\n        # Features need to be scaled in order to use same bandwidth throughout.\n        self.scaled_dataset = self._scale_features()\n\n    def calc_mutual_info_to_target(self, save_result: bool = True) -&gt; None:\n        \"\"\"\n        Calculate the mutual information between each feature and the target.\n        The target variable should be continuous.\n        Note that Sklearns implementation (used here) is designed for \"raw datasets\"\n        (i.e., do not feed in a probability distribution, instead feed in the observations).\n\n        Further, the mutual information values calculated from Sklearns implementation are\n        scaled by the natural logarithm of 2. In this implementation,\n        the results are re-scaled to be linear.\n\n        Parameters\n        ----------\n\n        save_result : Optional[bool] = True\n            Save result to disk or not.\n            Optional, default is to save.\n        \"\"\"\n        df_features = self.scaled_dataset.drop(\"Target\", axis=1)\n        features_array = df_features.to_numpy()\n        target_values = self.scaled_dataset[\"Target\"].to_numpy()\n\n        mutual_info_raw = mutual_info_regression(features_array, target_values)\n        mutual_info_rescaled = np.around((np.exp(mutual_info_raw) - 1), 5)\n\n        self.mutual_infos = dict(zip(df_features.columns, mutual_info_rescaled, strict=True))\n        self.mutual_infos = {k: v for k, v in sorted(self.mutual_infos.items(), key=lambda item: item[1], reverse=True)}\n\n        print(\"Mutual information scores calculated.\")\n\n        if save_result:\n            out_file_path = Path(self.out_dir, \"Mutual_Information_Per_Feature_Scores.csv\")\n            self._per_feature_scores_to_file(per_feat_values=self.mutual_infos, out_file=out_file_path)\n\n            print(\"You can also access these results via the class attribute: 'mutual_infos'.\")\n\n    def calc_linear_correl_to_target(self, save_result: bool = True) -&gt; None:\n        \"\"\"\n        Calculate the pearson correlation coeffcient between each feature and the target.\n\n        Parameters\n        ----------\n\n        save_result : Optional[bool] = True\n            Save result to disk or not.\n            Optional, default is to save.\n        \"\"\"\n        target = self.dataset[\"Target\"]\n        features = self.dataset.drop([\"Target\"], axis=1)\n\n        correlations = features.corrwith(target).to_frame()\n        sorted_correlations = correlations.sort_values(by=0, key=abs, ascending=False)\n        self.linear_correlations = sorted_correlations.to_dict(orient=\"dict\")[0]\n\n        print(\"Linear correlations calculated.\")\n\n        if save_result:\n            out_file_path = Path(self.out_dir, \"Linear_Correlations_Per_Feature_Scores.csv\")\n            self._per_feature_scores_to_file(per_feat_values=self.linear_correlations, out_file=out_file_path)\n\n            print(\"You can also access these results via the class attribute: 'linear_correlations'.\")\n</code></pre>"},{"location":"stat_modelling/#key_interactions_finder.stat_modelling.RegressionStatModel.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Filter and rescale the features.</p> Source code in <code>key_interactions_finder/stat_modelling.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Filter and rescale the features.\"\"\"\n    self.out_dir = _prep_out_dir(self.out_dir)\n\n    self.mutual_infos = {}\n    self.linear_correlations = {}\n\n    if sorted(self.interaction_types_included) != sorted([\"Hbond\", \"Hydrophobic\", \"Saltbr\", \"Other\"]):\n        self.dataset = _filter_features_by_strings(\n            dataset=self.dataset, strings_to_preserve=self.interaction_types_included\n        )\n\n    self.feature_list = list(self.dataset.columns)\n    self.feature_list.remove(\"Target\")\n\n    # Features need to be scaled in order to use same bandwidth throughout.\n    self.scaled_dataset = self._scale_features()\n</code></pre>"},{"location":"stat_modelling/#key_interactions_finder.stat_modelling.RegressionStatModel.calc_linear_correl_to_target","title":"<code>calc_linear_correl_to_target(save_result=True)</code>","text":"<p>Calculate the pearson correlation coeffcient between each feature and the target.</p> <p>Parameters:</p> Name Type Description Default <code>save_result</code> <code>Optional[bool] = True</code> <p>Save result to disk or not. Optional, default is to save.</p> <code>True</code> Source code in <code>key_interactions_finder/stat_modelling.py</code> <pre><code>def calc_linear_correl_to_target(self, save_result: bool = True) -&gt; None:\n    \"\"\"\n    Calculate the pearson correlation coeffcient between each feature and the target.\n\n    Parameters\n    ----------\n\n    save_result : Optional[bool] = True\n        Save result to disk or not.\n        Optional, default is to save.\n    \"\"\"\n    target = self.dataset[\"Target\"]\n    features = self.dataset.drop([\"Target\"], axis=1)\n\n    correlations = features.corrwith(target).to_frame()\n    sorted_correlations = correlations.sort_values(by=0, key=abs, ascending=False)\n    self.linear_correlations = sorted_correlations.to_dict(orient=\"dict\")[0]\n\n    print(\"Linear correlations calculated.\")\n\n    if save_result:\n        out_file_path = Path(self.out_dir, \"Linear_Correlations_Per_Feature_Scores.csv\")\n        self._per_feature_scores_to_file(per_feat_values=self.linear_correlations, out_file=out_file_path)\n\n        print(\"You can also access these results via the class attribute: 'linear_correlations'.\")\n</code></pre>"},{"location":"stat_modelling/#key_interactions_finder.stat_modelling.RegressionStatModel.calc_mutual_info_to_target","title":"<code>calc_mutual_info_to_target(save_result=True)</code>","text":"<p>Calculate the mutual information between each feature and the target. The target variable should be continuous. Note that Sklearns implementation (used here) is designed for \"raw datasets\" (i.e., do not feed in a probability distribution, instead feed in the observations).</p> <p>Further, the mutual information values calculated from Sklearns implementation are scaled by the natural logarithm of 2. In this implementation, the results are re-scaled to be linear.</p> <p>Parameters:</p> Name Type Description Default <code>save_result</code> <code>Optional[bool] = True</code> <p>Save result to disk or not. Optional, default is to save.</p> <code>True</code> Source code in <code>key_interactions_finder/stat_modelling.py</code> <pre><code>def calc_mutual_info_to_target(self, save_result: bool = True) -&gt; None:\n    \"\"\"\n    Calculate the mutual information between each feature and the target.\n    The target variable should be continuous.\n    Note that Sklearns implementation (used here) is designed for \"raw datasets\"\n    (i.e., do not feed in a probability distribution, instead feed in the observations).\n\n    Further, the mutual information values calculated from Sklearns implementation are\n    scaled by the natural logarithm of 2. In this implementation,\n    the results are re-scaled to be linear.\n\n    Parameters\n    ----------\n\n    save_result : Optional[bool] = True\n        Save result to disk or not.\n        Optional, default is to save.\n    \"\"\"\n    df_features = self.scaled_dataset.drop(\"Target\", axis=1)\n    features_array = df_features.to_numpy()\n    target_values = self.scaled_dataset[\"Target\"].to_numpy()\n\n    mutual_info_raw = mutual_info_regression(features_array, target_values)\n    mutual_info_rescaled = np.around((np.exp(mutual_info_raw) - 1), 5)\n\n    self.mutual_infos = dict(zip(df_features.columns, mutual_info_rescaled, strict=True))\n    self.mutual_infos = {k: v for k, v in sorted(self.mutual_infos.items(), key=lambda item: item[1], reverse=True)}\n\n    print(\"Mutual information scores calculated.\")\n\n    if save_result:\n        out_file_path = Path(self.out_dir, \"Mutual_Information_Per_Feature_Scores.csv\")\n        self._per_feature_scores_to_file(per_feat_values=self.mutual_infos, out_file=out_file_path)\n\n        print(\"You can also access these results via the class attribute: 'mutual_infos'.\")\n</code></pre>"},{"location":"utils/","title":"Utils","text":"<p>Random helper functions.</p> <p>Functions Available:</p> <p>per_residue_distance_to_site()     Calculate the closest heavy atom distance of each residue to an mdtraj defined     selection of a site of interest. You can write the results to file if desired.     Optionally can choose to only calculate minimum side chain distances.</p> <p>download_prep_tutorial_dataset()     Download one of the tutorial datasets from google drive using gdown and unzip it.</p>"},{"location":"utils/#key_interactions_finder.utils--the-functions-below-should-not-be-called-directly-by-an-end-user","title":"The functions below should not be called directly by an end user","text":"<p>_prep_out_dir()     Makes the folder if it doesn't exist and appends a '/' if not present at end of a string.</p> <p>_filter_features_by_strings()     Filter features to only include those that match one of the strings in the list provided.</p>"},{"location":"utils/#key_interactions_finder.utils.download_prep_tutorial_dataset","title":"<code>download_prep_tutorial_dataset(drive_url, save_dir)</code>","text":"<p>Download one of the tutorial datasets from google drive using gdown and unzip it.</p> <p>Parameters:</p> Name Type Description Default <code>drive_url</code> <code>str</code> <p>Google drive url to download the zip file from. Checked to see if a tutorial file.</p> required <code>save_dir</code> <code>str</code> <p>Directory to save and unpack the tutorial files.</p> required Source code in <code>key_interactions_finder/utils.py</code> <pre><code>def download_prep_tutorial_dataset(drive_url: str, save_dir: str) -&gt; None:\n    \"\"\"\n    Download one of the tutorial datasets from google drive using gdown and unzip it.\n\n    Parameters\n    ----------\n\n    drive_url : str\n        Google drive url to download the zip file from. Checked to see if a tutorial file.\n\n    save_dir : str\n        Directory to save and unpack the tutorial files.\n    \"\"\"\n    # tutorial links\n    accepted_links = [\n        \"https://drive.google.com/file/d/1hJbwCCuTTgI4xglwu1vXyzo-yaZJbmUY/view?usp=share_link\",\n        \"https://drive.google.com/file/d/13SjTIbSjF4ai_-Fn1vJDvW6-nsPmCdbw/view?usp=share_link\",\n        \"https://drive.google.com/file/d/1nbK3fw7z1hDXiGINZe-VT1SGdPbNhF07/view?usp=share_link\",\n        \"https://drive.google.com/file/d/1sYr_9stXLrOi_SDzHYLazSZUuzjL7lK-/view?usp=share_link\",\n        \"https://drive.google.com/file/d/10DbX12ZNPKqRIAlqs55HC8I6zLXJnP_q/view?usp=share_link\",\n        \"https://drive.google.com/file/d/1wPH4jOFOgIlpySLMN2ebk5PWzYgsrja2/view?usp=share_link\",\n        \"https://drive.google.com/file/d/1G4n-CXoqtt_qZtDfDXbByJtMTpbeIA4l/view?usp=share_link\",\n        \"https://drive.google.com/file/d/1pqFUMMjt9gDYOxtkVpDmyXwKXiHp0wFi/view?usp=share_link\",\n    ]\n\n    if drive_url not in accepted_links:\n        raise ValueError(\"You seem to be trying to download a non-tutorial file, stopping for safety.\")\n\n    # Prep the save_dir.\n    if save_dir != \"\":\n        save_dir_path = Path(save_dir)\n        if not save_dir_path.exists():\n            Path.mkdir(save_dir_path)\n    else:\n        save_dir_path = Path(\"\")\n\n    zip_file_path = Path(save_dir_path, \"tutorial_dataset.zip\")\n\n    # gdown does not accept Path objects.\n    gdown.download(url=drive_url, output=str(zip_file_path), quiet=False, fuzzy=True)\n\n    shutil.unpack_archive(filename=zip_file_path, extract_dir=save_dir_path, format=\"zip\")\n\n    print(\"Tutorial files were successfully downloaded and unzipped.\")\n</code></pre>"},{"location":"utils/#key_interactions_finder.utils.per_residue_distance_to_site","title":"<code>per_residue_distance_to_site(pdb_file, site_defintion, first_residue, last_residue, side_chain_only=False, out_file=None)</code>","text":"<p>Calculate the closest heavy atom distance of each residue to an mdtraj defined selection of a site of interest. You can write the results to file if desired.</p> <p>Parameters:</p> Name Type Description Default <code>pdb_file</code> <code>str</code> <p>Path to pdb file to use for the distance calculation.</p> required <code>site_defintion</code> <code>str</code> <p>mdtraj compatable defintion of the site of interest (i.e. binding site, active site etc..) See here for examples: https://mdtraj.org/1.9.3/atom_selection.html</p> required <code>first_residue</code> <code>int</code> <p>First residue to measure the distance from.</p> required <code>last_residue</code> <code>int</code> <p>Last residue to measure the distance to.</p> required <code>side_chain_only</code> <code>bool</code> <p>Choose whether you want to measure the minimum distance using only the side chain of each residue. If true, only the side chain atoms are used. For glycines (no side chain), the CA of the glycine is used instead.</p> <code>False</code> <code>out_file</code> <code>Optional[str]</code> <p>Path to output file to write out data.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Residue numbers are the keys and minimum distances are the values.</p> Source code in <code>key_interactions_finder/utils.py</code> <pre><code>def per_residue_distance_to_site(\n    pdb_file: str,\n    site_defintion: str,\n    first_residue: int,\n    last_residue: int,\n    side_chain_only: bool = False,\n    out_file: Optional[str] = None,\n) -&gt; dict:\n    \"\"\"\n    Calculate the closest heavy atom distance of each residue to an mdtraj defined\n    selection of a site of interest. You can write the results to file if desired.\n\n    Parameters\n    ----------\n\n    pdb_file : str\n        Path to pdb file to use for the distance calculation.\n\n    site_defintion : str\n        mdtraj compatable defintion of the site of interest\n        (i.e. binding site, active site etc..)\n        See here for examples: https://mdtraj.org/1.9.3/atom_selection.html\n\n    first_residue : int\n        First residue to measure the distance from.\n\n    last_residue : int\n        Last residue to measure the distance to.\n\n    side_chain_only: bool = False,\n        Choose whether you want to measure the minimum distance using only the\n        side chain of each residue. If true, only the side chain atoms are used.\n        For glycines (no side chain), the CA of the glycine is used instead.\n\n    out_file : Optional[str]\n        Path to output file to write out data.\n\n    Returns\n    ----------\n\n    dict\n        Residue numbers are the keys and minimum distances are the values.\n    \"\"\"\n    universe = mda.Universe(pdb_file)\n    group2 = universe.select_atoms(site_defintion)\n    min_dists = {}\n\n    if side_chain_only:\n        for residue in range(first_residue, last_residue + 1):\n            selection_str = \"not backbone and not name H* and resid \" + str(residue)\n            group1 = universe.select_atoms(selection_str)\n\n            res_dist_arr = distances.distance_array(group1.positions, group2.positions, box=universe.dimensions)\n\n            try:\n                min_res_dist = np.round(res_dist_arr.min(), 2)\n\n            # catches \"zero-size array to reduction operation minimum which has no identity\"\n            except ValueError:\n                # This happens for glycines which have no side chain...\n                selection_str = \"name CA and resid \" + str(residue)\n                group1 = universe.select_atoms(selection_str)\n\n                res_dist_arr = distances.distance_array(group1.positions, group2.positions, box=universe.dimensions)\n\n                min_res_dist = np.round(res_dist_arr.min(), 2)\n\n            min_dists.update({residue: min_res_dist})\n\n    else:  # both side and main chain route.\n        for residue in range(first_residue, last_residue + 1):\n            selection_str = \"not name H* and resid \" + str(residue)\n            group1 = universe.select_atoms(selection_str)\n\n            res_dist_arr = distances.distance_array(group1.positions, group2.positions, box=universe.dimensions)\n\n            min_res_dist = np.round(res_dist_arr.min(), 2)\n            min_dists.update({residue: min_res_dist})\n\n    if out_file is None:\n        return min_dists\n\n    with open(out_file, \"w\", newline=\"\", encoding=\"utf-8\") as file_out:\n        csv_out = csv.writer(file_out)\n        csv_out.writerow([\"Residue Number\", \"Minimum Distance\"])\n        csv_out.writerows(min_dists.items())\n        print(f\"{out_file} written to disk.\")\n    return min_dists\n</code></pre>"}]}